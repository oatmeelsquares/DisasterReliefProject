---
title: 'Disaster Relief Project, Part 1'
author: "Becky Desrosiers, Abner Casillas-Colon, Rachel Daniel"
date: "2024-03-16"
output: pdf_document
---

```{r r-setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      autodep = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE
                      )
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(discrim)
library(patchwork)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```


# Introduction

The 2010 earthquake in Haiti was a devastating natural disaster that caused extreme damage and displaced millions of people. After this disaster, the rescue workers needed to deliver food, water, and other resources to the people of Haiti, but it was challenging locating the people in need over this large area. These challenges included roads blocked by rubble and the inability to communicate because of damage to communication infrastructure. The rescue workers needed to find another strategy other than physically looking for them on land to locate and reach these displaced persons quickly and more efficiently. 

The people of Haiti who were displaced by the earthquake were using blue tarps as temporary shelter. This knowledge was utilized to locate these people after imagery was collected by aircraft flown by a rescue team from the Rochester Institute of Technology. Blue tarps could be searched for within these images by the rescue team who would then go to these coordinates and find them. However, that strategy would have been too slow, and resources would not have been delivered in time. A different strategy that could be used to more efficiently locate people in need of resources was to continue to use these images but instead use data-mining algorithms to search these images. 

This report explores various classification methods that could be useful in locating these blue tarps within the images taken by aircraft. The algorithms tested and explored include those that utilize Logistic Regression with and without penalty, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and K-Nearest Neighbors. We expect at least one of these methods to work more efficiently and more accurately than human efforts as these algorithms can handle more data, perform complex operations, and potentially fit to the patterns of the data to identify the blue tarps. The chosen algorithm will likely to miss fewer blue tarps than a human would. It is critical to identify that algorithm that will perform the best so that aid can be delivered to displaced people in dire circumstances. This is an important data-mining problem that could have a large impact on human life. This report will identify the algorithm that most accurately and most efficiently identifies these blue tarps so that the displaced Haitians can receive desperately needed resources in time. 


# Data

```{r load-data, include = FALSE}
# load data from file
file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
#file = '../data.csv'
data <- read_csv(file) %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
```

The dataset that will be used consists of class as a response variable and red, blue, green pixels as the predictor variables. There are a total of 63,241 records with no missing values. Pixels can have a range of 0-255 but for this dataset all pixels have a max of 255. Red and green pixels share a minimum value of 48 and blue has a minimum of 44. For Class there are five distinct values: Blue Tarp, Rooftop, Soil, Various Non-Tarp, and Vegetation.

The box plots below showcase the distribution of the range of pixel values grouped by each of the categories of images observed. For are analysis one thing we would like to note is the potential groups that could overlap with out goal value of blue tarps. Rooftop values seem to be the closet for which a potential prediction error is likely to occur as its values overlap quite closely with the blue tarps. This is followed by Various Non-Tarps that have a large range of values which encompass tarps.

```{r EDA_Boxplots}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each colour and class.
#| dev: "png"
#| dpi: 100
g1_box <- ggplot(data, aes(x=Class, y=Red))+
  geom_boxplot()+
  labs(title= "Boxplots of Red Pixel Value by Class")

g2_box <- ggplot(data, aes(x=Class, y=Green))+
  geom_boxplot()+
  labs(title= "Boxplots of Green Pixel Value by Class")

g3_box <- ggplot(data, aes(x=Class, y=Blue))+
  geom_boxplot()+
  labs(title= "Boxplots of Blue Pixel Value by Class")

g1_box/g2_box/g3_box

```

The countplot below shows the total counts grouped by class. Blue tarp has the smallest amount of values at 2022. While this is important to note for the threshold selection later on in the model. It is promising that the two largest values by far are vegetation and soil as they are the two who are least likely to represent false positive or false negative values when predicting for tarps. Rooftops and Various Non-Tarps however, do account for 14,647 which do have closer overlaps.  

```{r EDA_Countplot_Class_ungrouped }
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of total counts broken out by each class
#| dev: "png"
#| dpi: 100
ggplot(data, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Class", x="Count")
```
Table 1 highlights the values of Tarps vs Non-Tarp. In this instance we note that the proportion of tarps in the total dataset is 3%. When we conduct our threshold selection across all models this will be important as the models will default to a 50-50 threshold.


```{r EDA_Count_Table_1}
data <- data %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
table(data$Class) %>% knitr::kable(caption = "Count of Class")
```

# Description of Methodology

This model is created using RStudio with the following packages: tidyverse, tidymodels, ggcorrplot, GGally, patchwork, discrim and doParallel. RStudio is an open-source statistical analysis software that offers a variety of packages to assist in model building and model selection. The first step of this analysis is to change the classifier into our variable of interest. The raw data classifies the pixels into Blue Tarp, Rooftop, Soil, Various Non-Tarp, and Vegetation, however we are only interested in whether the pixel is a Blue Tarp or not a Blue Tarp. We changed the factor outcome variable to have only two levels: Tarp and Non-Tarp. As the goal of this model will be to predict the class of a combination of pixels the following classification models will be explored: Logistic Regression, Penalized Logistic Regression (a logistic regression model with tuning parameters for an elastic net), linear discriminant analysis (LDA), quadtratic discriminant analysis (QDA), and a K-Nearest-Neighbors (KNN) model.

Of these models, the KNN model and the Logistic Regression Model will utilize tuning parameters. Tuning parameters will be selected to maximize ROC AUC. For the KNN model, the selected tuning parameter is the number of neighbors, k. The number of neighbors selected will be the lowest k within one standard deviation of the maximum ROC AUC. For the penalized model, mixture and penalty will be tuned, and the best model will be selected based on ROC AUC.

For all models a tenfold cross-validation will be used for model validation. All metrics will be estimated using tenfold cross-validation. With an overall sample size of 63,241, this allows for each fold to have 6,324 records. With this amount of data, a tenfold cross validation is a reliable method to evaluate the performance of the training sets for all models. We set the seed for all random processes to 6030.

All models will be compared based on ROC AUC. This metric provides a method to evaluate the performance of the True Positive Rate and False Positive Rate across all possible thresholds from 0 to 1. With its efficacy as an overall evaluator of performance, this metric will be used for model selection when comparing all versions of the tenfold validation models. 

As established during the EDA, the proportion of Blue Tarps to all other classes is approximately 3%. This indicates that a 50% threshold would be a poor choice for the final model. While five metrics will be explored for threshold selection (accuracy, j-index, sensitivity, specificity, and precision) our primary metrics of interest will be sensitivity and j-index. Sensitivity is especially important because we want to minimize false negatives - those could represent individuals that could be missed when response time is extremely important. The j-index is chosen as a metric that balances sensitivity and specificity. This will be valuable in models were responders have limited resources to check potential points of interest. While maximizing based on j-index will result in a higher false negative rate, we will greatly reduce false positives, which will make sure that we can make the most out of the time and resources allocated to the relief effort. In this report, we will assume that we have relatively unlimited resources and therefore pick thresholds based rimarily on sensitivity. J-index will be our secondary metric for selecting threshold. These same metrics will be used and weighted similarly when determining model performance, for similar reasons.



# Results of Model Fitting, Tuning Parameter Selection, and Evaluation


```{r convenience-functions}
# define functions to use later for convenience
cv_control <- control_resamples(save_pred = TRUE)

roc_plot <- function(model_preds) {
  model_preds %>% 
    roc_curve(truth = Class,
              .pred_Tarp,
              event_level = 'second') %>% 
    autoplot()
}

threshold_metric_plot <- function(thresh_perf, max_sens, max_j) {
  ggplot(thresh_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
    geom_line() +
    geom_vline(data = max_sens, aes(xintercept = .threshold, color = .metric)) +
    geom_vline(data = max_j, aes(xintercept = .threshold, color = .metric)) +
    scale_x_continuous(breaks = seq(0, 1, 0.1)) +
    labs(x = 'Threshold', y = 'Metric value')
}

threshold_metrics <- metric_set(j_index,
                                sensitivity,
                                specificity,
                                accuracy,
                                precision)


threshold_preds <- function(preds, threshold) {
    preds <- preds %>% 
    mutate(.pred_class = factor(ifelse(.pred_Tarp >= threshold, 'Tarp', 'Non-Tarp')))
}

```


```{r model-setup}
# prepare resamples for 10-fold cross-validation
set.seed(6030)
resamples <- vfold_cv(data, v = 10, strata=Class)

# define formula
formula <- Class ~ Red + Green + Blue

# define basic recipe
rec <- recipe(formula, data = data)
```


## K-Nearest Neighbors


```{r knn}
## PART I ##
# define model spec
knn_spec <- nearest_neighbor(mode = 'classification',
                            engine = 'kknn',
                            neighbors = parsnip::tune())

# define workflow
knn_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(knn_spec)

# set tuning parameters
knn_params <- extract_parameter_set_dials(knn_wf) %>% 
  update(neighbors = neighbors(c(2, 100)))

# tune with grid (or fit resamples)
knn_tune <- tune_grid(knn_wf,
                      resamples = resamples,
                      grid = grid_regular(knn_params, levels = 50),
                      control = cv_control
                      )

# get tuning results visualization
knn_tune_vis <- autoplot(knn_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
knn_best_params <- select_best(knn_tune, neighbors, metric = 'roc_auc')

knn_final <- knn_wf %>% 
  finalize_workflow(knn_best_params)              ## !!

knn_fitcv <- fit_resamples(knn_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
knn_cv_preds <-  collect_predictions(knn_fitcv) ## UPDATE VARIABLE NAMES
knn_cv_metrics <- collect_metrics(knn_fitcv)    ## !!

# get ROC plot
knn_cv_roc <- roc_plot(knn_preds)   ## !!

# get threshold selection info
knn_thresh_perf <- probably::threshold_perf(knn_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
knn_max_sens <- knn_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
knn_max_j <- knn_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
knn_tmetrics_plot <- threshold_metric_plot(knn_thresh_perf, knn_max_sens, knn_max_j)

# get predictions and metrics based on chosen threshold
knn_threshold_preds <- knn_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.04, 'Tarp', 'Non-Tarp')))

knn_threshold_metrics <- threshold_metrics(knn_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
knn_train_cm <- conf_mat(knn_threshold_preds, truth = Class, estimate = .pred_class) ## !!

## PART II ##
# get predictions and metrics for holdout data
knn_test_preds <- augment(knn_final, new_data = holdout)
knn_test_roc <- roc_plot(knn_test_preds)

knn_test_metrics
```


We will now build a K-nearest neighbors (KNN) model with the number of neighbors, k, tuned to find the best fit. Figure 3 shows the ROC AUC of the model when fitted with different numbers of neighbors. It shows that the AUC of the ROC curve increases with k, maxing around 50, and leveling out at higher tuning values. The highest AUC is **0.9943** and belongs to the model with k = 40 neighbors. However, we choose a model within one standard deviation of the best metric for simplicity. From the plot, we can see that the point with the lowest number of neighbors above the dashed standard-error line represents the model with k = 8 nearest neighbors. We will use this tuning metric moving forward.


```{r figure-X-knn-tune}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC AUC based on number of principal components and nearest neighbors. The dashed horizontal line represents one standard deviation lower than the best AUC metric.
#| dev: "png"
#| dpi: 100
knn_tune_vis
```

With the number of neighbors tuned to k = 8, our model has an ROC AUC of **0.9933**. The ROC curve will be displayed in the next section, when all of the models are compared. The next step is to select the appropriate threshold for our model. Figure 4 shows five different metrics plotted against the threshold value, with thresholds ranging from 0.01 to 0.99. The metrics were calculated using tenfold cross-validation. The vertical lines show that sensitivity and j-index are both maximized at 0.02, 0.03, and 0.04 equally. Therefore, since the other metrics increase with the threshold, we choose the highest one, threshold = 0.04, to move forward with.

```{r figure-X-knn-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the KNN model with k = 8 nearest neighbors. Vertical lines show maximum j-index and sensitivity between 0.02 and 0.04.
#| dev: "png"
#| dpi: 100
knn_tmetrics_plot
```

Our final model has k = 8 nearest neighbors and a selection threshold of 0.04. The corresponding confusion matrix is shown below.

```{r knn-conf-mat}
knn_cm
```

The confusion matrix shows that our final model misses only 26 Blue Tarp pixels and falsely identifies 251 Non-Tarp pixels. The overall accuracy of the model is 0.9956, with sensitivity of 0.9959. The precision is also very high: 0.9996. The specificity is 0.9871 and the j-index comes out to 0.9830.


## Linear Discriminant Analysis

```{r lda}
# define model spec
lda_spec <- discrim_linear(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
lda_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(lda_spec)


# fit resamples
lda_fitcv <- fit_resamples(lda_wf, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
lda_preds <-  collect_predictions(lda_fitcv)
lda_metrics <- collect_metrics(lda_fitcv)

lda_roc <- roc_plot(lda_preds)

# get threshold selection info
lda_thresh_perf <- probably::threshold_perf(lda_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))

# get threshold for best sensitivity
lda_max_sens <- lda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
lda_max_j <- lda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
lda_tmetrics_plot <- threshold_metric_plot(lda_thresh_perf, lda_max_sens, lda_max_j)

# get predictions and metrics based on chosen threshold
lda_threshold_preds <- lda_preds %>% 
 mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.01, 'Tarp', 'Non-Tarp')))

lda_threshold_metrics <- threshold_metrics(lda_threshold_preds, truth = Class, estimate = .pred_class)


# get confusion matrix for chosen threshold
lda_cm <- conf_mat(lda_threshold_preds, truth = Class, estimate = .pred_class)

```

We built and trained the LDA model using our data set and then assessed the model’s performance metrics. We found that the model has an AUC of 0.9888, which means that it performs well on the data and classifies blue tarp pixels versus non-tarp pixels better than a random model. The next step was choosing the appropriate threshold for this model. Figure 5 displays the threshold plot with the plotted values of the chosen metrics from 0 to 1.   

```{r figure-X-lda-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Four metrics based on threshold value for the LDA model. Vertical lines show maximum j-index and sensitivity at 0.01.
#| dev: "png"
#| dpi: 100
lda_tmetrics_plot
```
 

These displayed metrics are accuracy, j-index, sensitivity, and specificity, but we are most interested in sensitivity and j-index. Both sensitivity and j-index perform the best at a threshold of around 0.01. This value of 0.01 will be the chosen threshold for our model when utilizing it for predictions. After threshold selection, the confusion matrix for the model was assessed where we explored the number of false negatives, false positives, and true positives. We found that while we miss some blue tarp pixels, we locate many more, while also limiting the number of false positives.  

```{r lda-cm}
lda_cm
```

 
The LDA model performed very well on the data at the selected threshold. Sensitivity, accuracy, and precision are all estimated between 0.976 and 0.996. This model appears to be fairly accurate and sensitive to the data.  

## Quadratic Discriminant Analysis

```{r qda}
# define model spec
qda_spec <- discrim_quad(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
qda_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(qda_spec)


#fit resamples
qda_fitcv <- fit_resamples(qda_wf, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
qda_preds <-  collect_predictions(qda_fitcv)
qda_metrics <- collect_metrics(qda_fitcv)

qda_roc <- roc_plot(qda_preds)

# get threshold selection info
qda_thresh_perf <- probably::threshold_perf(qda_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))

# get threshold for best sensitivity
qda_max_sens <- qda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
qda_max_j <- qda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
qda_tmetrics_plot <- threshold_metric_plot(qda_thresh_perf, qda_max_sens, qda_max_j)


# get predictions and metrics based on chosen threshold
qda_threshold_preds <- qda_preds %>% 
 mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.015, 'Tarp', 'Non-Tarp')))

qda_threshold_metrics <- threshold_metrics(qda_threshold_preds, truth = Class, estimate = .pred_class)


# get confusion matrix for chosen threshold
qda_cm <- conf_mat(qda_threshold_preds, truth = Class, estimate = .pred_class)
```


We then built and trained a QDA model using the same blue tarp and non-tarp data set. We found that the model's AUC was 0.9982, which means it performs a lot better than a random model. The next step was selecting the best threshold based on sensitivity and j_index. We found that the maximum sensitivity is at a threshold of 0.01, while the maximum j-index was at 0.02. These metrics at each threshold value are shown within the threshold plot in figure 6. 


```{r figure-X-qda-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Four metrics based on threshold value for the QDA model. Vertical lines show maximum j-index and sensitivity at 0.02 and 0.01, respectively.
#| dev: "png"
#| dpi: 100
qda_tmetrics_plot
```
 

Since sensitivity and j-index are both important for this data, we decided to choose a threshold in between 0.01 and 0.02 for this model, which was 0.015. This threshold value seems to maximize both metrics, as the j-index is 0.9692, while the sensitivity is 0.9717.  

After we selected the appropriate threshold for our model, we then assessed the confusion matrix with the selected threshold. The model performed extremely well at this selected threshold in terms of locating tarp pixels as there are only 5 that are missed. However, when choosing a final model, it will be important to keep in mind that the number of false positives is fairly high and may impact our final model choice. 

```{r qda-cm}
qda_cm
```

The QDA model at the selected threshold has a sensitivity of 0.9717, accuracy of 0.9725, and precision of 0.9999. These metrics reveal that the QDA model performs very well on the data and can correctly identify many of the tarps. The QDA model appears to perform better than the LDA model so it seems that the data benefits from some flexibility in the predictive algorithm. 


## Logistic Regression Analysis and Penalized Logistic Regression Analysis

```{r logistic_regression}
log_spec <- logistic_reg(mode = 'classification',
                            engine = 'glm')
                            
# define workflow
log_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(log_spec)


# tune with grid (or fit resamples)

log_fitcv <-fit_resamples(log_wf, resamples, control=cv_control)

# collect predictions and roc_auc from cross-validated fit
log_preds <-  collect_predictions(log_fitcv)
log_metrics <- collect_metrics(log_fitcv)

# get ROC plot
log_roc <- roc_plot(log_preds)

# get threshold selection info
log_thresh_perf <- probably::threshold_perf(log_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))

# get threshold for best sensitivity
log_max_sens <- log_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
log_max_j <- log_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
log_tmetrics_plot <- threshold_metric_plot(log_thresh_perf, log_max_sens, log_max_j)

# get predictions and metrics based on chosen threshold
log_threshold_preds <- log_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.04, 'Tarp', 'Non-Tarp')))

log_threshold_metrics <- threshold_metrics(log_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
log_cm <- conf_mat(log_threshold_preds, truth = Class, estimate = .pred_class)
```


```{r penalized_logistic}
# define model spec
pen_spec <- logistic_reg(mode = 'classification',
                            engine = 'glmnet',
                            penalty = tune(),
                            mixture = tune())
                            
# define workflow
pen_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(pen_spec)

# set tuning parameters
pen_params <- extract_parameter_set_dials(pen_wf) %>% 
  update(
    penalty=penalty(c(-20,-1)),
    mixture=mixture(c(0,1))
  )

# tune with grid (or fit resamples)
pen_tune <- tune_grid(pen_wf,
                      resamples=resamples,
                      control=cv_control,
                      grid=grid_latin_hypercube(pen_params, size=50))

# get tuning results visualization
pen_tune_vis <- autoplot(pen_tune, metric = "roc_auc")

# finalize workflow and fit resamples with best parameters
pen_best_params <- select_best(pen_tune, metric="roc_auc")

pen_fitcv <- pen_wf %>% 
  finalize_workflow(pen_best_params) %>% 
  fit_resamples(resamples,
                control = cv_control
                )

# collect predictions and roc_auc from cross-validated fit
pen_preds <-  collect_predictions(pen_fitcv)
pen_metrics <- collect_metrics(pen_fitcv)

# get ROC plot
pen_roc <- roc_plot(pen_preds)

# get threshold selection info
pen_thresh_perf <- probably::threshold_perf(pen_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))

# get threshold for best sensitivity
pen_max_sens <- pen_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
pen_max_j <- pen_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
pen_tmetrics_plot <- threshold_metric_plot(pen_thresh_perf, pen_max_sens, pen_max_j)

# get predictions and metrics based on chosen threshold
# the optimal threshold for the sens is .01
pen_threshold_preds <- pen_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.04, 'Tarp', 'Non-Tarp')))

pen_threshold_metrics <- threshold_metrics(pen_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
pen_cm <- conf_mat(pen_threshold_preds, truth = Class, estimate = .pred_class)
```

There are two types of logistic regression models that are used, a standard logistic regression model and a penalized logistic regression model. While the logistic regression model does not have any parameters to tune, the penalized logistic regression model is a model that applies a penalty value on predictor variables to attempt to shrink less important variables.
There are two tuning parameters of interest mixture, which is a metric that will determine if the model is closer to a ridge regression or lasso regression; and penalty, which measures how strongly the predictor variables are penalized. For this model the optimal mixture is 0.208 and a penalty of 1.279e-10. When examining figure 7 we can see how larger penalty values incur worse performance. These values reflect how important all predictor variables are for identifying the class of the value. The ridge regression model does not reduce the values to zero like lasso, this reflects why the mixture value is relatively low. The penalty value being small reflects the importance of all variables and how reducing all values proportionally does not benefit the model to a large degree.

```{r figure-X-pen-tune}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC based on penalty and mixture values. Left hand chart corresponds to penalty value, right hand chart corresponds to mixture.
#| dev: "png"
#| dpi: 100
pen_tune_vis
```
Our two metrics of interest for threshold selection are the j-index and sensitivity which can be seen in figures 8 and 9. The j-index represents a metric that captures the overall performance of the classification model at capturing both true negatives and true positives. Sensitivity is a measure that prioritizes minimizing false negatives. Depending on the circumstances both measures present important considerations for threshold selection. The j-index is ideal if trying to find a balance of overall performance; for our models this would be helpful when we have limited resources for first responders to try and go through many false positives. Maximizing sensitivity allows is ideal if there are large number of resources for first responders to ensure that no blue tarps are missed due to false negatives. For the logistic regression model the optimal threshold that maximizes the j-index is .05 and for sensitivity it is .01. For the penalized logistic regression model the optimal threshold that maximizes the j-index is .04 and for sensitivity it is .01. As the overall split of blue tarp vs non blue tarp is 3%, we will utilize the .05 threshold for the logistic regression and .04 for the penalized logistic regression.  Both these values maximize the j-index to find more of a balance between sensitivity and specificity, substantially reducing the number of false positives compared to the .01 threshold.

```{r figure-X-log-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Logistic Regression model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.05.
#| dev: "png"
#| dpi: 100
log_tmetrics_plot
```

```{r figure-X-pen-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Penalized Logistic Regression model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.04.
#| dev: "png"
#| dpi: 100
pen_tmetrics_plot
```

The confusion matrices below shows the breakout of our values at the optimal thresholds for the Logistic Regression and Peanalized Logistic Regression Model respecively.

```{r log-conf-mat}
log_cm
```

```{r pen-conf-mat}
pen_cm
```

To evaluate final model performance, we examine the models ROC AUC as a metric that captures its full performance across all thresholds. This flexibility allows leeway in case of changing threshold selection for prioritizing a different goal outcome. The AUC of the optimal logistic regression model is 0.9985 which indicates a very high performing model. The AUC of the optimal penalized logistic regression model is 0.9986 which also is a very high value.



```{r compare-models}
aucs <- ( log_roc + pen_roc  + knn_roc) / ( lda_roc + qda_roc )

compare_metrics <- bind_rows(log_metrics %>% mutate(model = 'logistic regression'),
                             pen_metrics %>% mutate(model = 'penalized logreg'),
                             lda_metrics %>% mutate(model = 'LDA'),
                             qda_metrics %>% mutate(model = 'QDA'),
                             knn_metrics %>% mutate(model = 'KNN')
                             ) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = mean) %>% 
  dplyr::select(model, roc_auc)

compare_threshold_metrics <- bind_rows(log_threshold_metrics %>% mutate(model = 'logistic regression'),
                             pen_threshold_metrics %>% mutate(model = 'penalized logreg'),
                             lda_threshold_metrics %>% mutate(model = 'LDA'),
                             qda_threshold_metrics %>% mutate(model = 'QDA'),
                             knn_threshold_metrics %>% mutate(model = 'KNN')
                             ) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = .estimate) %>% 
  mutate(threshold = c(.05, .04, .01, .015, .04)) 

```

# Results

When making a final selection and recommendation of what model to use we first begin by evaluating the ROC AUC of all the models.  Figure 10 shows the ROC curves with Table 2 showing the AUC metric for each model. All of the ROC curves show excellent performance, which is confirmed by the AUC values. Every model has a very high AUC value with the lowest being the LDA at .9888 and the highest being the penalized logistic regression at .9986 (all within 1% of each other).  While the ROC does offer good insight into overall performance it does not provide the whole picture as to final model performance. 

```{r figure-X-ROC-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC plots for all models from top row Logistic, Penalized, KNN  Bottom row LDA, QDA
#| dev: "png"
#| dpi: 100
aucs
```

```{r table-2-metrics}
compare_metrics %>% knitr::kable(caption = "Compare AUC of all models")
```

Table 3 shows the other metrics for each model at a given threshold (refer to section 3.5 for a discussion of threshold selection). All of the models perform similarly with KNN having the highest j-index and sensitivity. The j-index ranges from 0.8648 for the LDA to 0.9830 for the KNN model.  The sensitivity ranges from 0.9659 for the QDA to 0.9959 for the KNN model. 

```{r table-3-metrics}
compare_threshold_metrics %>% knitr::kable(caption = "Compare Threshold Metrics of all models")
```



# Conclusion



## Conclusion 1

We concluded that the KNN method results in the best algorithm that classifies each pixel as a blue tarp pixel or a non-tarp pixel after the performance metrics for each model were assessed. While it is important to consider all metrics when determining the best model, in this context, it is most important to identify the majority of the tarp pixels and limit the number of false negatives so that we can locate most of the displaced persons. The sensitivity and j-index were the metrics that we mainly focused on with this goal in mind. The KNN model had the highest j-index and the highest sensitivity out of all of the models so that is the one that we believe will be most useful and accurate in identifying blue tarps. This model will play a major role in providing resources to displaced persons and potentially saving human lives.

## Conclusion 2

The goal of this modeling is ultimately to assist in predicting blue tarps to assist humanitarian efforts in the result of a natural disaster.  With that in mind, one area that presented a fascinating part of follow on research was the threshold selection. We ultimately selected the model with an optimal J-index but for any sort of model selection coordinating with a response team is crucial to determining a final threshold. Natural disasters often necessitate extremely fast response time as the first 24 hours are vital in saving lives. With that in mind, we asserted that resources are somewhat limited and balancing false positives and false negatives was more valuable than committing to reducing false negatives. In real world scenarios, developing a model in conjunction with subject matter experts is integral to the process in ensuring that effective prioritization of metrics is done to align with the objective of the analysis.


## Conclusion 3

We found that all of the methods performed very well, and there was no one clearly best method. Though the KNN model had the best sensitivity and j-index, which we considered to be the most important metrics, all of the models had very high metrics for not only j-index and sensitivity, but specificity, accuracy and precision as well. For this reason, we are more confident in our results because the models were able to reliably perform well on the data.






# Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
stopCluster(cl)
registerDoSEQ()
```
