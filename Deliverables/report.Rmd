---
title: 'Disaster Relief Project, Part 1'
author: "Becky Desrosiers, Abner Casillas-Colon, Rachel Daniel"
date: "2024-03-16"
output: pdf_document
---

```{r r-setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      autodep = TRUE,
                      message = FALSE,
                      warning = FALSE)
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(discrim)
library(patchwork)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```


# Section 1: Introduction

The 2010 earthquake in Haiti was a devastating natural disaster that caused extreme damage and displaced millions of people. After this disaster, the rescue workers needed to deliver food, water, and other resources to the people of Haiti, but it was challenging locating the people in need over this large area. These challenges included roads blocked by rubble and the inability to communicate because of damage to communication infrastructure. The rescue workers needed to find another strategy other than physically looking for them on land to locate and reach these displaced persons quickly and more efficiently. 

The people of Haiti who were displaced by the earthquake were using blue tarps as temporary shelter. This knowledge was utilized to locate these people after imagery was collected by aircraft flown by a rescue team from the Rochester Institute of Technology. Blue tarps could be searched for within these images by the rescue team who would then go to these coordinates and find them. However, that strategy would have been too slow, and resources would not have been delivered in time. A different strategy that could be used to more efficiently locate people in need of resources was to continue to use these images but instead use data-mining algorithms to search these images. 

This report explores various classification methods that could be useful in locating these blue tarps within the images taken by aircraft. The algorithms tested and explored include those that utilize Logistic Regression with and without penalty, Linear Discriminant Analysis, Quadratic Discriminant Analysis, and K-Nearest Neighbors. We expect at least one of these methods to work more efficiently and more accurately than human efforts as these algorithms can handle more data, perform complex operations, and potentially fit to the patterns of the data to identify the blue tarps. The chosen algorithm will likely to miss fewer blue tarps than a human would. It is critical to identify that algorithm that will perform the best so that aid can be delivered to displaced people in dire circumstances. This is an important data-mining problem that could have a large impact on human life. This report will identify the algorithm that most accurately and most efficiently identifies these blue tarps so that the displaced Haitians can receive desperately needed resources in time. 


# Section 2: Data

```{r load-data, warning = FALSE, message = FALSE}
# load data from file
# file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
file = '../data.csv'
data <- read_csv(file) %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))

data %>% glimpse()
data %>% summary()
```
>You will need to do some EDA
- EDA - focus your discussion of the results on what it means for the model
- Be selective with your EDA. Focus on results that have a relevance to the project. You can always mention that you tried other visualizations, but didn’t get much more out of it. If you add a graph, it should add to your analysis and cover something that cannot already be seen in other graphs.





# Section 3: Description of Methodology

This model is created using RStudio with the following packages: tidyverse, tidymodels, ggcorrplot, GGally, patchwork, discrim and doParallel. RStudio is an open-source statistical analysis software that offers a variety of packages to assist in model building and model selection. The first step of this analysis is to change the classifier into our variable of interest. The raw data classifies the pixels into Blue Tarp, Rooftop, Soil, Various Non-Tarp, and Vegetation, however we are only interested in whether the pixel is a Blue Tarp or not a Blue Tarp. We changed the factor outcome variable to have only two levels: Tarp and Non-Tarp. As the goal of this model will be to predict the class of a combination of pixels the following classification models will be explored: Logistic Regression, Penalized Logistic Regression (a logistic regression model with tuning parameters for an elastic net), linear discriminant analysis (LDA), quadtratic discriminant analysis (QDA), and a K-Nearest-Neighbors (KNN) model.

Of these models, the KNN model and the Logistic Regression Model will utilize tuning parameters. Tuning parameters will be selected to maximuze ROC AUC. For the KNN model, the selected tuning parameter is the number of neighbors, k. The number of neighbors selected will be the lowest k within one standard deviation of the maximum ROC AUC. For the penalized model, mixture and penalty will be tuned, and the best model will be selected based on ROC AUC.

For all models a tenfold cross-validation will be used for model validation. All metrics will be estimated using tenfold cross-validation. With an overall sample size of 63,241, this allows for each fold to have 6,324 records. With this amount of data, a tenfold cross validation is a reliable method to evaluate the performance of the training sets for all models.

All models will be compared based on ROC AUC. This metric provides a method to evaluate the performance of the True Positive Rate and False Positive Rate across all possible thresholds from 0 to 1. With its efficacy as an overall evaluator of performance, this metric will be used for model selection when comparing all versions of the tenfold validation models. 

As established during the EDA, the proportion of Blue Tarps to all other classes is approximately 3%. This indicates that a 50% threshold would be a poor choice for the final model. While five metrics will be explored for threshold selection (accuracy, j-index, sensitivity, specificity, and precision) our primary metrics of interest will be sensitivity and j-index. Sensitivity is especially important because we want to minimize false negatives - those could represent individuals that could be missed when response time is extremely important. The j-index is chosen as a metric that balances sensitivity and specificity. This will be valuable in models were responders have limited resources to check potential points of interest. While maximizing based on j-index will result in a higher false negative rate, we will greatly reduce false positives, which will make sure that we can make the most out of the time and resources allocated to the relief effort. In this report, we will assume that we have relatively unlimited resources and therefore pick thresholds based rimarily on sensitivity. J-index will be our secondary metric for selecting threshold. These same metrics will be used and weighted similarly when determining model performance, for similar reasons.



# Section 4: Results of Model Fitting, Tuning Parameter Selection, and Evaluation


>Model performance summarized in one or more tables or figures. Expected information shown:
• ROC curves and AUC
• Optimal model tuning parameters
• Selected threshold
• Accuracy, TPR, FPR, Precision calculated at selected threshold

>When you present metrics, talk about their relevance for the objective of this project
- When you present your results, split it into results from model validation and results after threshold selection. e.g. ROC curves are only relevant for discussing model validation results and not threshold selection; reporting accuracy or any other threshold dependent metric prior to threshold selection is irrelevant.
- If you visualize the same analysis for each model, combine them either in a single graph or a combination of graphs in one figure (using patchwork)

>Focus your discussion also on differences between models.
- For example LDA and QDA are conceptually similar but differ in the way they can represent the decision boundary. 
- What do the differences in the results for LDA and QDA tell you?
- Another example that you can discuss is the difference between , penalized logistic regression and logistic regression.





```{r convenience-functions}
# define functions to use later for convenience
cv_control <- control_resamples(save_pred = TRUE)

roc_plot <- function(model_preds) {
  model_preds %>% 
    roc_curve(truth = Class,
              .pred_Tarp,
              event_level = 'second') %>% 
    autoplot()
}

threshold_metric_plot <- function(thresh_perf, max_sens, max_j) {
  ggplot(thresh_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
    geom_line() +
    geom_vline(data = max_sens, aes(xintercept = .threshold, color = .metric)) +
    geom_vline(data = max_j, aes(xintercept = .threshold, color = .metric)) +
    scale_x_continuous(breaks = seq(0, 1, 0.1)) +
    labs(x = 'Threshold', y = 'Metric value')
}

threshold_metrics <- metric_set(j_index,
                                specificity,
                                sensitivity,
                                accuracy,
                                precision)


threshold_preds <- function(preds, threshold) {
    preds <- preds %>% 
    mutate(.pred_class = factor(ifelse(.pred_Tarp >= threshold, 'Tarp', 'Non-Tarp')))
}

```


```{r model-setup}
# prepare resamples for 10-fold cross-validation
set.seed(6030)
resamples <- vfold_cv(data, v = 10, strata=Class)

# define formula
formula <- Class ~ Red + Green + Blue

# define basic recipe
rec <- recipe(formula, data = data)
```


## Section 3.5.5: K-Nearest Neighbors


```{r knn}
# define model spec
knn_spec <- nearest_neighbor(mode = 'classification',
                            engine = 'kknn',
                            neighbors = parsnip::tune())

# define workflow
knn_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(knn_spec)

# set tuning parameters
knn_params <- extract_parameter_set_dials(knn_wf) %>% 
  update(neighbors = neighbors(c(2, 100)))

# tune with grid (or fit resamples)
knn_tune <- tune_grid(knn_wf,
                      resamples = resamples,
                      grid = grid_regular(knn_params, levels = 50),
                      control = cv_control
                      )

# get tuning results visualization
knn_tune_vis <- autoplot(knn_tune, metric = 'roc_auc') +
  geom_hline(yintercept = (0.9943110 - 0.001139833), linetype = 'dashed')

# finalize workflow and fit resamples with best parameters
knn_best_params <- select_by_one_std_err(knn_tune, neighbors, metric = 'roc_auc')

knn_fitcv <- knn_wf %>% 
  finalize_workflow(knn_best_params) %>% 
  fit_resamples(resamples,
                control = cv_control
                )

# collect predictions and roc_auc from cross-validated fit
knn_preds <-  collect_predictions(knn_fitcv)
knn_metrics <- collect_metrics(knn_fitcv)

# get ROC plot
knn_roc <- roc_plot(knn_preds)

# get threshold selection info
knn_thresh_perf <- probably::threshold_perf(knn_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
knn_max_sens <- knn_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
knn_max_j <- knn_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
knn_tmetrics_plot <- threshold_metric_plot(knn_thresh_perf, knn_max_sens, knn_max_j)

# get predictions and metrics based on chosen threshold
knn_threshold_preds <- knn_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.04, 'Tarp', 'Non-Tarp')))

knn_threshold_metrics <- threshold_metrics(knn_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
knn_cm <- conf_mat(knn_threshold_preds, truth = Class, estimate = .pred_class)
```


We will now build a K-nearest neighbors (KNN) model with the number of neighbors, k, tuned to find the best fit. Figure X shows the ROC AUC of the model when fitted with different numbers of neighbors. It shows that the AUC of the ROC curve increases with k, maxing around 50, and leveling out at higher tuning values. The highest AUC is **0.9943** and belongs to the model with k = 40 neighbors. However, we choose a model within one standard deviation of the best metric for simplicity. From the plot, we can see that the point with the lowest number of neighbors above the dashed standard-error line represents the model with k = 8 nearest neighbors. We will use this tuning metric moving forward.


```{r figure-X-knn-tune}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC AUC based on number of principal components and nearest neighbors. The dashed horizontal line represents one standard deviation lower than the best AUC metric.
#| dev: "png"
#| dpi: 100
knn_tune_vis
```

With the number of neighbors tuned to k = 8, our model has an ROC AUC of **0.9933**. The ROC curve will be displayed in the next section, when all of the models are compared. The next step is to select the appropriate threshold for our model. Figure X shows five different metrics plotted against the threshold value, with thresholds ranging from 0.01 to 0.99. The metrics were calculated using tenfold cross-validation. The vertical lines show that sensitivity and j-index are both maximized at 0.02, 0.03, and 0.04 equally. Therefore, since the other metrics increase with the threshold, we choose the highest one, threshold = 0.04, to move forward with.

```{r figure-X-knn-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the KNN model with k = 8 nearest neighbors. Vertical lines show maximum j-index and sensitivity between 0.02 and 0.04.
#| dev: "png"
#| dpi: 100
knn_tmetrics_plot
```

Our final model has k = 8 nearest neighbors and a selection threshold of 0.04. The corresponding confusion matrix is shown below.

```{r knn-conf-mat}
knn_cm
```

The confusion matrix shows that our final model misses only 26 Blue Tarp pixels and falsely identifies 251 Non-Tarp pixels. The overall accuracy of the model is 0.9956, with sensitivity of 0.9959. The precision is also very high: 0.9996. The specificity is 0.9871 and the j-index comes out to 0.9830.


# Linear Discriminant Analysis

```{r lda}
# define model spec
lda_spec <- discrim_linear(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
lda_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(lda_spec)


# fit resamples
lda_fitcv <- fit_resamples(lda_wf, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
lda_preds <-  collect_predictions(lda_fitcv)
lda_metrics <- collect_metrics(lda_fitcv)

# get ROC plot (will just talk about ROC in write up without graph/listed metrics)
lda_roc <- roc_plot(lda_preds)

# get threshold selection info
lda_thresh_perf <- probably::threshold_perf(lda_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))

# get threshold for best sensitivity
lda_max_sens <- lda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
lda_max_j <- lda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
lda_tmetrics_plot <- threshold_metric_plot(lda_thresh_perf, lda_max_sens, lda_max_j)

# get predictions and metrics based on chosen threshold
lda_threshold_preds <- lda_preds %>% 
 mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.01, 'Tarp', 'Non-Tarp')))

lda_threshold_metrics <- threshold_metrics(lda_threshold_preds, truth = Class, estimate = .pred_class)
lda_threshold_metrics

# get confusion matrix for chosen threshold
lda_cm <- conf_mat(lda_threshold_preds, truth = Class, estimate = .pred_class)

```

# Quadratic Discriminant Analysis

```{r qda}
# define model spec
qda_spec <- discrim_quad(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
qda_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(qda_spec)


#fit resamples
qda_fitcv <- fit_resamples(qda_wf, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
qda_preds <-  collect_predictions(qda_fitcv)
qda_metrics <- collect_metrics(qda_fitcv)

# get ROC plot (will just talk about ROC in write up without graph/listed metrics)
qda_roc <- roc_plot(qda_preds)

# get threshold selection info
qda_thresh_perf <- probably::threshold_perf(qda_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))

# get threshold for best sensitivity
qda_max_sens <- qda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
qda_max_j <- qda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
qda_tmetrics_plot <- threshold_metric_plot(qda_thresh_perf, qda_max_sens, qda_max_j)


# get predictions and metrics based on chosen threshold
qda_threshold_preds <- qda_preds %>% 
 mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.01, 'Tarp', 'Non-Tarp')))

qda_threshold_metrics <- threshold_metrics(qda_threshold_preds, truth = Class, estimate = .pred_class)


# get confusion matrix for chosen threshold
qda_cm <- conf_mat(qda_threshold_preds, truth = Class, estimate = .pred_class)
```










```{r compare-models}
aucs <- ( log_roc + penlog_roc  + knn_roc) / ( lda_roc + qda_roc )

compare_metrics <- bind_rows(log_threshold_metrics %>% mutate(model = 'logistic regression'),
                             penlog_threshold_metrics %>% mutate(model = 'penalized logreg'),
                             lda_threshold_metrics %>% mutate(model = 'LDA'),
                             qda_threshold_metrics %>% mutate(model = 'QDA'),
                             knn_threshold_metrics %>% mutate(model = 'KNN')
                             ) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = .estimate)
```





# Section 5: Conclusion


>Three or more clearly identifiable conclusions. This section is more important than the previous sections (as
reflected in the points). Give sufficient explanation and justification for each conclusion.
One conclusion must be:
• determination and justification of which algorithm works best.
Additional conclusions should be observations you’ve made based on your work on this project, such as:
• What additional recommend actions can be taken to improve results?
• Were there multiple adequately performing methods, or just one clear best method? What is your level
of confidence in the results?
• What is it about this data formulation that allows us to address it with predictive modeling tools?
• How effective do you think your work here could actually be in terms of helping to save human life?
• Do these data seem particularly well-suited to one class of prediction methods, and if so, why?
These are only suggestions, pursue your own interests. Your best two additional conclusions will be graded.
Make sure that the 3 conclusions are clearly separated.










# Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
stopCluster(cl)
registerDoSEQ()
```
