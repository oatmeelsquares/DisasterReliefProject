---
title: "rachel-final-code"
output: html_document
date: "2024-03-14"
---

When knitting, we should add this and/or add all code to the end in the appendix.

```{r hide-code, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

# Appendix {-}
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```



```{r load-packages}
knitr::opts_chunk$set(cahce = TRUE, autodep = TRUE)
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(patchwork)
library(discrim)
library(dplyr)
```

## EDA would be before this



```{r Parallel processing}
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```


```{r load_data}
data <- read_csv("https://gedeck.github.io/DS-6030/project/HaitiPixels.csv")
```


```{r transform&mutate data}
data <- data %>% 
  mutate(
    Class = factor(ifelse(Class== "Blue Tarp", "Tarp", "Non-Tarp")))
```

```{r Split data}
set.seed(1)
data_split <- initial_split(data, prop=.8, strata=Class)
train <- training(data_split)
test <- testing(data_split)
```


This section explores the Linear Discriminant Analysis approach in determining the best model for identifying blue tarps within Haiti after the earthquake. This model was assessed using the computational engine "MASS" within the discrim package. The mode for LDA is always classification, and that makes sense here since we are attempting to classify each observation as either tarp or Non-Tarp. 

First the formula and recipe are defined and the workflow is created.

```{r Specify model}
#| cache: TRUE

# define formula
formula <- Class ~ Red + Green + Blue

# define recipe
data_recipe <- recipe(formula, data = train)

#define model
lda_spec <- discrim_linear(mode = "classification") %>% 
  set_engine('MASS')

#define workflow
lda_wf <- workflow() %>% 
  add_recipe(data_recipe) %>% 
  add_model(lda_spec)
```


The performance of the model was assessed using 10-fold cross validation. 10-fold cross-validation reduces the risk of our model overfitting by utilizing different values within the training set. The mean of the performance metrics gathered from the 10-fold cross-validation can more reliably be used to estimate model performance.

```{r CV and metrics}
# cross validation and metrics
resamples <- vfold_cv(train, v=10, strata=Class)
metrics <- metric_set(roc_auc,sensitivity,specificity,j_index,accuracy) #ok to take out everything but ROC if you want
cv_control <- control_resamples(save_pred=TRUE)

lda_cv <- fit_resamples(lda_wf, resamples, metrics=metrics, control=cv_control)
collect_metrics(lda_cv)
```
```{r LDA_ROC}
#Plotting ROC

roc_cv_plot <- function(model_cv, model_name) {
  cv_predictions <- collect_predictions(model_cv)
  cv_ROC <- cv_predictions %>%
    roc_curve(truth=Class, .pred_Tarp, event_level="second")
  autoplot(cv_ROC) +
    labs(title=model_name)
}
roc_lda <- roc_cv_plot(lda_cv, "LDA")
roc_lda
```

The performance metrics reveal that the LDA model performs pretty well on the training data. The ROC and sensitivity for this model are both pretty fairly high. This is further demonstrated by the ROC-AUC plot below. This model appears to do a great job of being able to identify Tarps over Non-Tarps, however it doesn't identify 324 observations as tarps at the current threshold. This number is larger than desired as it would not be great to miss 324 tarps that potentially contain people that need help. It would be best to decrease this number by decreasing the threshold.

```{r LDA conf_mat}
lda_pred <- collect_predictions(lda_cv)
# confusion matrix for threshold = 0.5
conf_mat(lda_pred, truth = Class, estimate = .pred_class)
```


In order to successfully identify tarps, it is best to choose the threshold based on sensitivity. The threshold plot reveals that the best threshold is below 0.25 based on both sensitivity and the j-index. 

```{r}
threshold_graph <- function(model_cv, model_name) {
  performance <- probably::threshold_perf(collect_predictions(model_cv), Class, .pred_Tarp,
                                          thresholds=seq(0.05, 0.95, 0.01), event_level="second",
                                          metrics=metric_set(j_index, accuracy,sensitivity,specificity))
  max_metrics <- performance %>%
    group_by(.metric) %>%
    filter(.estimate == max(.estimate))
  ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
    geom_line() +
    geom_point(data=max_metrics, color="black") +
    labs(x="Threshold", y="Metric value") +
    coord_cartesian(ylim=c(0, 1))
}
threshold_lda <- threshold_graph(lda_cv,"LDA")
threshold_lda
```



```{r lda-conf-mtx}
#new threshold
lda_conf_mtx <- lda_pred %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.01, 'Tarp', 'Non-Tarp')))
conf_mat(lda_conf_mtx, truth = Class, estimate = .pred_class)

```

It is best to have less false negatives in this situation as it is not great if people in need of help are missed. There is a trade off between specificity and sensitivity, and a threshold of 0.01 makes it so that false positive rate is only about 2%, while decreasing the number of false negatives. It is best to choose a threshold of 0.01 for this model when using it for prediction.

```{r}
# False positive rate
(1025/(1025+47954))*100
```


```{r}
# New threshold applied
lda_post <- 
  lda_pred %>%
  mutate(.pred = probably::make_two_class_pred(.pred_Tarp, levels(Class), threshold = 0.01))
```

```{r}
lda_final <- lda_wf %>% 
  fit(train)
```

```{r qda-test-metrics}
get_metrics <- metric_set(j_index, sensitivity, specificity, accuracy)

# get metrics on training data
lda_train_metrics <- lda_post %>% 
  get_metrics(truth = Class, estimate = .pred_class)

# get metrics on test data
lda_test_metrics <- parsnip::augment(lda_final, new_data = test) %>%
  get_metrics(truth = Class, estimate = .pred_class)

```

  
```{r QDA-compare-metrics}
bind_rows(lda_train_metrics %>% mutate(data ='Training'),
          lda_test_metrics %>%  mutate(data = 'Testing')) %>% 
  pivot_wider(id_cols =.metric,
              names_from = data,
              values_from = .estimate)

```


This model performs relatively well on this data. The j-index is slightly lower than desired, but the accuracy and sensitivity are both high. 



This section explores Quadratic Discriminant Analysis. QDA is a more flexible approach than LDA and may better utilize the large dataset available for this purpose.

Here, we build and define the QDA model and workflow.

```{r Build QDA}
#| cache: TRUE

# define formula
qda_formula <- Class ~ Red + Green + Blue

# define recipe
qda_data_recipe <- recipe(formula, data = train)

#define model
qda_spec <- discrim_quad(mode = "classification") %>% 
  set_engine('MASS')

#define workflow
qda_wf <- workflow() %>% 
  add_recipe(qda_data_recipe) %>% 
  add_model(qda_spec)
```


10-fold cross validation is also performed here on the training data.

```{r QDA CV and metrics}

resamples <- vfold_cv(train, v=10, strata=Class)
metrics <- metric_set(roc_auc,sensitivity,specificity,j_index,accuracy)
cv_control <- control_resamples(save_pred=TRUE)

qda_cv <- fit_resamples(qda_wf, resamples, metrics=metrics, control=cv_control)
collect_metrics(qda_cv)
```
The QDA model performs really well as the performance metrics are all large. 

```{r QDA_ROC}
#Plotting ROC
roc_qda <- roc_cv_plot(qda_cv, "QDA")
roc_qda
```
It is further evident from the ROC-AUC plot that this model is very successfulin identifying Tarp versus Non-Tarp observations.



```{r QDA confus_mat}
qda_pred <- collect_predictions(qda_cv)
#threshold = 0.5
conf_mat(qda_pred, truth = Class, estimate = .pred_class)
```
As demonstrated in the confusion matrix, a threshold of 0.5 is too high as the model is missing many Tarps that need to be identified. There is a lot less false positives so we can decrease the number of false negatives, while increasing the number of false positives without much consequence.

```{r}
threshold_qda <- threshold_graph(qda_cv,"QDA")
threshold_qda
```


It appears tha a threshold below 0.25 is best to maximize the j-index and sensitivity. This should also decrease the number of false negatives.


```{r qda-conf-mtx}
qda_conf_mtx <- qda_pred %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.01, 'Tarp', 'Non-Tarp')))
conf_mat(qda_conf_mtx, truth = Class, estimate = .pred_class)

```

```{r}
(1689/(1689+47290))*100
```

At a threshold of 0.01, there is only 1 false negative so only 1 tarp is missed. The false positives only make up 3.4% of the positives, which is a decent range. More people are provided with supplies without wasting too many search resources.

```{r}
# New threshold applied
qda_post <- 
  qda_pred %>%
  mutate(.pred = probably::make_two_class_pred(.pred_Tarp, levels(Class), threshold = 0.01))
```

```{r}
qda_final <- qda_wf %>% 
  fit(train)
```

```{r qda-test-metrics}
get_metrics <- metric_set(j_index, sensitivity, specificity, accuracy)

# get metrics on training data
qda_train_metrics <- qda_post %>% 
  get_metrics(truth = Class, estimate = .pred_class)

# get metrics on test data
qda_test_metrics <- parsnip::augment(qda_final, new_data = test) %>%
  get_metrics(truth = Class, estimate = .pred_class)

```

  
```{r QDA-compare-metrics}
bind_rows(qda_train_metrics %>% mutate(data ='Training'),
          qda_test_metrics %>%  mutate(data = 'Testing')) %>% 
  pivot_wider(id_cols =.metric,
              names_from = data,
              values_from = .estimate)
```

The QDA model performs very well on this data. The performance metrics are all very high for both the training and the testing data.  

```{r}
roc_qda + roc_lda
```
The QDA model performs better than the LDA model, which means that the model benefits from a more flexible approach.

```{r End parallel processing}
stopCluster(cl)
registerDoSEQ()
```




