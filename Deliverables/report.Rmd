---
title: 'Disaster Relief Project, Part 1'
author: "Becky Desrosiers, Abner Casillas-Colon, Rachel Daniel"
date: "2024-03-16"
output: pdf_document
---

```{r r-setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      autodep = TRUE,
                      message = FALSE,
                      warning = FALSE)
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(discrim)
library(patchwork)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```


# Section 1: Introduction

>What is the key idea for locating people and why do we expect it to work?



# Section 2: Data

```{r load-data, warning = FALSE, message = FALSE}
# load data from file
# file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
file = '../data.csv'
data <- read_csv(file) %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))

data %>% glimpse()
data %>% summary()
```
>You will need to do some EDA
- EDA - focus your discussion of the results on what it means for the model
- Be selective with your EDA. Focus on results that have a relevance to the project. You can always mention that you tried other visualizations, but didn’t get much more out of it. If you add a graph, it should add to your analysis and cover something that cannot already be seen in other graphs.





# Section 3: Description of Methodology

>Overall model building process well defined and explained.
• describe used software
• describe and justify parameter tuning and model selection (if applicable)
• describe and justify model validation
• describe and justify threshold selection
• describe and justify metrics used for model performance evaluation

>Model training consists of these steps:
- Model validation using 10-fold cross validation; this includes model tuning if applicable
- what metric will you use?
- Threshold selection
- what metric will you use?
- Are “penalized logistic regression” and “logistic regression” really two different models or just variations of the same?

From Becky: be sure to mention we did a 20-80 test-training split with stratified sampling of Blue Tarp pixels.




# Section 4: Results of Model Fitting, Tuning Parameter Selection, and Evaluation


>Model performance summarized in one or more tables or figures. Expected information shown:
• ROC curves and AUC
• Optimal model tuning parameters
• Selected threshold
• Accuracy, TPR, FPR, Precision calculated at selected threshold

>When you present metrics, talk about their relevance for the objective of this project
- When you present your results, split it into results from model validation and results after threshold selection. e.g. ROC curves are only relevant for discussing model validation results and not threshold selection; reporting accuracy or any other threshold dependent metric prior to threshold selection is irrelevant.
- If you visualize the same analysis for each model, combine them either in a single graph or a combination of graphs in one figure (using patchwork)

>Focus your discussion also on differences between models.
- For example LDA and QDA are conceptually similar but differ in the way they can represent the decision boundary. 
- What do the differences in the results for LDA and QDA tell you?
- Another example that you can discuss is the difference between , penalized logistic regression and logistic regression.





```{r convenience-functions}
# define variables and functions to use later for convenience
auc_metric <- metric_set(roc_auc)

cv_control <- control_resamples(save_pred = TRUE)

roc_plot <- function(model_fitcv) {
  model_fitcv %>% 
    collect_predictions() %>% 
    roc_curve(truth = Class,
              .pred_Tarp,
              event_level = 'second') %>% 
    autoplot()
}

threshold_metric_plot <- function(model_preds) {
  
  thresh_perf <- probably::threshold_perf(model_preds,
                                          Class,
                                          .pred_Tarp,
                                          thresholds = seq(0.01, 0.99, 0.01),
                                          event_level = 'second',
                                          metrics = threshold_metrics)

  max_sens <- thresh_perf %>% 
    filter(.metric == 'sensitivity') %>% 
    filter(.estimate == max(.estimate))

  max_j <- thresh_perf %>% 
    filter(.metric == 'j_index') %>% 
    filter(.estimate == max(.estimate))

  ggplot(thresh_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
    geom_line() +
    geom_vline(data = max_sens, aes(xintercept = .threshold, color = .metric)) +
    geom_vline(data = max_j, aes(xintercept = .threshold, color = .metric)) +
    scale_x_continuous(breaks = seq(0, 1, 0.1)) +
    labs(x = 'Threshold', y = 'Metric value')
}

threshold_metrics <- metric_set(j_index,
                                specificity,
                                sensitivity,
                                accuracy)

get_metrics <- function(final_model, data) {
  threshold_metrics(parsnip::augment(final_model, data),
                    truth = Class,
                    estimate = .pred_class)
}

metric_table <- function(final_model) {
  bind_rows(get_metrics(final_model, train) %>% 
              mutate(data = 'train'),
            get_metrics(final_model, test) %>% 
              mutate(data = 'test')
            ) %>% 
    pivot_wider(id_cols = data,
                names_from = .metric,
                values_from = .estimate) %>% 
    mutate(difference = test - train)
}

```


```{r model-setup}
# split data into training-test sets
set.seed(1)
data_split <- initial_split(data, prop = .8, strata = Class)
train <- training(data_split)
test <- testing(data_split)

# prepare resamples for 10-fold cross-validation
resamples <- vfold_cv(train, v=10, strata=Class)

# define formula
formula <- Class ~ Red + Green + Blue

# define basic recipe
rec <- recipe(formula, data = train)
```


## Section 4.1: Logistic Regression Model


```{r logreg, warning = FALSE, message = FALSE}
# define and execute cross validation workflow for logistic regression
logreg_spec <- logistic_reg(engine = "glm",
                            mode = "classification")

logreg_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(logreg_spec)

logreg_fitcv <- logreg_wf %>% 
  fit_resamples(resamples,
                control = cv_control)

# collect metrics and predictions from cross-validated fit
logreg_cv_metrics <- collect_metrics(logreg_fitcv) %>% 
  dplyr::select(c(.metric, mean))

logreg_preds <- collect_predictions(logreg_fitcv)

# collect metrics for finalized model
logreg_final <- logreg_wf %>% 
  fit(train)

logreg_train_metrics <- get_metrics(logreg_final, train)
logreg_test_metrics <- get_metrics(logreg_final, test)
```

The first model investigated by this report is a simple logistic regression with no penalty. Following the methodology specified, the model was defined and resamples were fitted to find the roc_auc.

```{r figure-1}
#| fig.width: 9
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Plot of ROC curve for the logistic regression model with no penalty. The AUC is 0.9976.
#| dev: "png"
#| dpi: 100
# display logreg ROC curve
roc_plot(logreg_fitcv)
```

We note a very high area under the curve (AUC) of **0.9976**, indicating a very effective model. Figure 1 shows the corresponding receiver operating characteristic (ROC) curve to demonstrate the performance of the model. From the visualization, it is clear that the model performs well. To optimize its predictive capability, we now investigate further to choose a threshold.


```{r figure-2}
#| fig.width:8
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Visualization of model metrics based on threshold, with vertical lines representing the maximum sensitivity and j-index.
#| dev: "png"
#| dpi: 100
# display threshold metric plot for logistic regression
threshold_metric_plot(logreg_preds)
```

```{r logreg-final-metrics}
# display metrics on test and training set

```

## Penalized Logistic Regression Model


Set up the penalized logistic regression model that will be tuned on both a penalty factor and a mixture for the model. Penalty parameter was experimented with until a range where the drop off was near the middle of the chart could be chosen.

```{r Model_Tuning_Logistic_Penalized}

tune_logreg_spec <- logistic_reg(engine="glmnet", mode="classification",
                                 penalty = tune(), mixture= tune())

tune_logreg_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(tune_logreg_spec)

logreg_params <- extract_parameter_set_dials(tune_logreg_wf) %>% 
  update(
    penalty=penalty(c(-20,-1)),
    mixture=mixture(c(0,1))
  )
```

Run the model across the cross validation folds and extract the three best versions of the model using the roc_auc as a metric of evaluation. We note that the best models tend to have a lower mixture (closer to a ridge regression penalty) and a low penalty negative penalty value for lambda. We note a drop off for regularization around -5, and fairly even lasso penalty after .1 with sporadic models under performing across the range.

```{r Cross_Validation_Tuned_Model_Logistic_Penalized}

tune_results_logreg <- tune_grid(tune_logreg_wf,
                                 resamples=resamples,
                                 control=cv_control,
                                 grid=grid_latin_hypercube(logreg_params, size=50))
show_best(tune_results_logreg, metric="roc_auc", n=3)
```

```{r plot_grid_results_Logistic_Penalized}
autoplot(tune_results_logreg)
```
Showing the metrics of the logistic regression model and comparing the roc_auc of the model to the penalized logistic regression model.

```{r tuned_parameter_selection}

best_params_logreg <- show_best(tune_results_logreg, metric="roc_auc")

roc_cv_plot_tune <- function(cv_predictions, model_name){
  cv_ROC <- cv_predictions %>% roc_curve(truth=Class, .pred_Tarp, event_level="second")
  autoplot(cv_ROC) +
    labs(title=model_name)
}

g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g1_tuned <- collect_predictions(tune_results_logreg,
                                parameters=best_params_logreg, summarize=TRUE) %>% 
  roc_cv_plot_tune("Logistic Regression Tuned")
g1/g1_tuned
```

Fit the penalized logistic regression model and showcase the final performance metrics of the logistic regression model vs the penalized logistic regression model. When examining the performance of the two models we note that the logistic regression model seems to perform better over the penalized logistic regression model.

```{r Finalize_the_models}

tuned_logreg_model <- tune_logreg_wf %>% 
  finalize_workflow(select_best(tune_results_logreg, metric="roc_auc")) %>% 
  fit(train)

tuned_logreg_model_cv <- tune_logreg_wf %>% 
  finalize_workflow(select_best(tune_results_logreg, metric="roc_auc")) %>% 
  fit_resamples(resamples, metrics=metric_evals, control=cv_control)

cv_metrics <- bind_rows(
  collect_metrics(logreg_cv) %>%
    mutate(model="Logistic regression untuned"),
  show_best(tune_results_logreg, metric="roc_auc", 1) %>%
    mutate(model="Logistic regression tuned"),
  
)
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean-std_err, xmax=mean+std_err))+
  geom_point()+
  geom_linerange()+
  facet_wrap(~ .metric)
```

```{r Final_ROC_AUC}

bind_rows(
  roc_auc(augment(logreg_untuned_model, test), Class, .pred_Tarp, event_level="second") %>%
    mutate(model="logreg untuned"),
  roc_auc(augment(tuned_logreg_model, test), Class, .pred_Tarp, event_level="second") %>%
    mutate(model="logreg tuned"),
) %>%
ggplot(aes(x=.estimate, y=model)) +
  geom_point() +
  facet_wrap(~ .metric)
```



```{r logistic-cm}

log_preds <- collect_predictions(logreg_cv)
penalized_log_preds <- collect_predictions(tuned_logreg_model_cv) 
```

```{r log_threshold}


penal_log_thres_perf <- probably::threshold_perf(penalized_log_preds,
                                           Class,
                                           .pred_Tarp,
                                           threshold=seq(0.01,0.99,0.01),
                                           event_level= "second",
                                           metrics= metric_set(j_index,
                                                               specificity,
                                                               sensitivity,
                                                               accuracy))
```

```{r log_metric_plot}

max_sens_log <- log_thres_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

max_j_log <- log_thres_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

max_sens_pen <- penal_log_thres_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

max_j_pen <- penal_log_thres_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))
  

```

The threshold plots for these models show that for both models when maximizing the sensitivity they share the same threshold at .01. We are interested in maximizing sensitivity as time to first response is the most important thing in disaster scenarios. For this circumstance that means that we want to set the thresholds to the .01. Additionally, while this decision does sacrifice total performance the j-index for both metrics still performs at .94 for the log model and .90 for the penalized log model reflecting a respectable score overall.
```{r log_threshold_plots}

ggplot(log_thres_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
  geom_line() +
  geom_vline(data = max_sens_log, aes(xintercept = .threshold, color = .metric)) +
  geom_vline(data = max_j_log, aes(xintercept = .threshold, color = .metric)) +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  labs(title= "Logistic Model",x = 'Threshold', y = 'Metric value')

ggplot(log_thres_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
  geom_line() +
  geom_vline(data = max_sens_pen, aes(xintercept = .threshold, color = .metric)) +
  geom_vline(data = max_j_pen, aes(xintercept = .threshold, color = .metric)) +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  labs(title= "Penalized Logistic Model", x = 'Threshold', y = 'Metric value')

```
The confusion matrix when the thresholds are set to maximizing the sensitivity for both metrics results in only 12 false negatives for the base logistic model and 2 for the penalized logistic regression model. Depending on the circumstances it may be worthwhile to examine a more balanced threshold which is done by selecting the J-index max below. These would be more useful if resources are limited and the rescue team does not have the resources to full examine all options. This does however result in more false negatives with 37 and 34 respectively that could be potentially damaging to response efforts.
```{r log_conf_matrix}

# Confusion Matrices for the Logistic Regression Models
log_preds <- log_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.01, 'Tarp', 'Other')))
conf_mat(log_preds, truth = Class, estimate = .pred_class)

log_preds <- log_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.5, 'Tarp', 'Other')))
conf_mat(log_preds, truth = Class, estimate = .pred_class)

log_preds <- log_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.04, 'Tarp', 'Other')))
conf_mat(log_preds, truth = Class, estimate = .pred_class)

writeLines("\n\n")

# Confusion Matrices for the penalized logistic regresion models
penalized_log_preds <- penalized_log_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.01, 'Tarp', 'Other')))
conf_mat(penalized_log_preds, truth = Class, estimate = .pred_class)

penalized_log_preds <- penalized_log_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.5, 'Tarp', 'Other')))
conf_mat(penalized_log_preds, truth = Class, estimate = .pred_class)

penalized_log_preds <- penalized_log_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.05, 'Tarp', 'Other')))
conf_mat(penalized_log_preds, truth = Class, estimate = .pred_class)



```

```{r log_metrics_training}

get_metrics <- metric_set(j_index, sensitivity, specificity, accuracy)

# Get Metrics on train data
log_train_metrics <- log_preds %>% 
  get_metrics(truth= Class, estimate = .pred_class)

pen_log_train_metrics <- penalized_log_preds %>% 
  get_metrics(truth= Class, estimate = .pred_class)

# Get Metrics on test data

log_test_metrics <- parsnip::augment(logreg_untuned_model, new_data = test) %>% 
  get_metrics(truth = Class, estimate = .pred_class)

pen_log_test_metrics <- parsnip::augment(tuned_logreg_model, new_data = test) %>% 
  get_metrics(truth = Class, estimate = .pred_class)

```

When evaluating the final model we note that the penalized and non penalized model both have very similar scores of sensitivity of .999 with their testing sets. While the models are similar in this regard all other metrics are outperformed by the base model rather than the penalized model. While our goal should be to maximize the sensitivity going with the Logistic Regression model will allow us more flexibility when it comes to instances where we are interested in more balanced outputs. For that reason we would select the Logistic Regression model over the Penalized Logistic Regression Model as our final ideal model.

```{r final_metrics_comparison}

bind_rows(log_train_metrics %>% mutate(data ="Log Training"),
          log_test_metrics %>% mutate(data = "Log Testing"),
          pen_log_train_metrics %>% mutate(data = "Penalized Log Training"),
          pen_log_test_metrics %>% mutate(data = "Penalized Log Testing")) %>% 
  pivot_wider(id_cols = .metric,
              names_from = data,
              values_from = .estimate)


```

















# Section 5: Conclusion


>Three or more clearly identifiable conclusions. This section is more important than the previous sections (as
reflected in the points). Give sufficient explanation and justification for each conclusion.
One conclusion must be:
• determination and justification of which algorithm works best.
Additional conclusions should be observations you’ve made based on your work on this project, such as:
• What additional recommend actions can be taken to improve results?
• Were there multiple adequately performing methods, or just one clear best method? What is your level
of confidence in the results?
• What is it about this data formulation that allows us to address it with predictive modeling tools?
• How effective do you think your work here could actually be in terms of helping to save human life?
• Do these data seem particularly well-suited to one class of prediction methods, and if so, why?
These are only suggestions, pursue your own interests. Your best two additional conclusions will be graded.
Make sure that the 3 conclusions are clearly separated.










# Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
stopCluster(cl)
registerDoSEQ()
```
