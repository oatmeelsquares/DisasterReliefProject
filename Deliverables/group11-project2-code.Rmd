---
title: 'Disaster Relief Project, Part 1'
author: "Becky Desrosiers, Abner Casillas-Colon, Rachel Daniel"
date: "2024-04-22"
---

# Calculations

```{r r-setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      autodep = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE
                      )
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(discrim)
library(patchwork)
library(doParallel)
library(kernlab)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```

```{r load-data, include = FALSE}
# load data from file
file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
#file = '../data.csv'
data <- read_csv(file) #%>% 
  #mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
```

```{r holdout processing EDA}
columns = c('ID', 'X','Y','Map X','Map Y','Lat','Lon','B1','B2','B3')

data_67_BT <- read_table("orthovnir067_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_57_NON <- read_table("orthovnir057_ROI_NON_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_67_NOT <- read_table("orthovnir067_ROI_NOT_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_69_NOT <- read_table("orthovnir069_ROI_NOT_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_69_bt <- read_table("orthovnir069_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_78_bt <- read_table("orthovnir078_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_78_NON <- read_table("orthovnir078_ROI_NON_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))
```

```{r holdout EDA}
data_full <- bind_rows(
  data_67_BT,
  data_57_NON,
  data_67_NOT,
  data_69_NOT,
  data_69_bt,
  data_78_bt,
  data_78_NON) %>% 
  rename('Red' = 'B1',
         'Green' = 'B2',
         'Blue' = 'B3') %>% 
  mutate(Class = factor(Class, levels = c('Non-Tarp', 'Tarp')))

holdout <- data_full %>% select(c(Class, Red, Green, Blue))
```

```{r convenience-functions}
# define functions to use later for convenience
cv_control <- control_resamples(save_pred = TRUE)

roc_plot <- function(model_preds, model_name) {
  roc <- model_preds %>% 
    roc_curve(truth = Class,
              .pred_Tarp,
              event_level = 'second') %>% 
    autoplot()
    zoom <- roc + coord_cartesian(xlim = c(0, 0.1), ylim = c(0.9, 1)) +
    theme(axis.title.x=element_blank(), #remove x axis labels
          axis.text.x = element_blank(),
          axis.title.y=element_blank()  #remove y axis labels
          )
  roc +
    labs(title = paste(model_name)) +
    inset_element(zoom, left = 0.4, right = 0.95, bottom = 0.05, top = 0.6)
}


threshold_metric_plot <- function(thresh_perf, max_sens, max_j) {
  ggplot(thresh_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
    geom_line() +
    geom_vline(data = max_sens, aes(xintercept = .threshold, color = .metric)) +
    geom_vline(data = max_j, aes(xintercept = .threshold, color = .metric)) +
    scale_x_continuous(breaks = seq(0, 1, 0.1)) +
    labs(x = 'Threshold', y = 'Metric value')
}

threshold_metrics <- metric_set(j_index,
                                sensitivity,
                                specificity,
                                accuracy,
                                precision)

test_metrics <- function(model_test_preds) {
  bind_rows(
    roc_auc(model_test_preds,
            truth = Class,
            .pred_Tarp,
            event_level = 'second'),
    threshold_metrics(model_test_preds,
                      truth = Class,
                      estimate = .pred_class,
                      )
  )
}


threshold_preds <- function(preds, threshold) {
    preds <- preds %>% 
    mutate(.pred_class = factor(ifelse(.pred_Tarp <= threshold, 'Non-Tarp', 'Tarp')))
     
}

```

```{r model-setup}
# prepare resamples for 10-fold cross-validation
set.seed(6030)
resamples <- vfold_cv(data, v = 10, strata=Class)

# define formula
formula <- Class ~ Red + Green + Blue

# define basic recipe
rec <- recipe(formula, data = data)
```

```{r knn}
## PART I ##
# define model spec
knn_spec <- nearest_neighbor(mode = 'classification',
                            engine = 'kknn',
                            neighbors = parsnip::tune())

# define workflow
knn_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(knn_spec)

# set tuning parameters
knn_params <- extract_parameter_set_dials(knn_wf) %>% 
  update(neighbors = neighbors(c(2, 100)))

# tune with grid (or fit resamples)
knn_tune <- tune_grid(knn_wf,
                      resamples = resamples,
                      grid = grid_regular(knn_params, levels = 50),
                      control = cv_control
                      )

# get tuning results visualization
knn_tune_vis <- autoplot(knn_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
knn_best_params <- select_best(knn_tune, metric = 'roc_auc')

knn_final <- knn_wf %>% 
  finalize_workflow(knn_best_params)

knn_fitcv <- fit_resamples(knn_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
knn_cv_preds <-  collect_predictions(knn_fitcv)
knn_cv_metrics <- collect_metrics(knn_fitcv)

# get ROC plot for cross-validation
knn_cv_roc <- roc_plot(knn_cv_preds, 'KNN')

# get threshold selection info
knn_thresh_perf <- probably::threshold_perf(knn_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
knn_max_sens <- knn_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
knn_max_j <- knn_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
knn_tmetrics_plot <- threshold_metric_plot(knn_thresh_perf, knn_max_sens, knn_max_j)

# get best threshold
knn_threshold <- as.numeric(knn_max_j[1, 1])

# get predictions and metrics based on chosen threshold
knn_threshold_preds <- threshold_preds(knn_cv_preds, knn_threshold)

knn_threshold_metrics <- threshold_metrics(knn_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
knn_train_cm <- conf_mat(knn_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##
# fit final workflow to training data
knn_fit <- fit(knn_final, data)

# get predictions and metrics for holdout data
knn_test_preds <- threshold_preds(augment(knn_fit, new_data = holdout),
                                  knn_threshold)
knn_test_roc <- roc_plot(knn_test_preds, 'KNN')

knn_test_metrics <- test_metrics(knn_test_preds)

knn_test_cm <- conf_mat(knn_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r lda}
# define model spec
lda_spec <- discrim_linear(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
lda_final <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(lda_spec)

# fit resamples
lda_fitcv <- fit_resamples(lda_final, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
lda_cv_preds <-  collect_predictions(lda_fitcv)
lda_cv_metrics <- collect_metrics(lda_fitcv)

lda_cv_roc <- roc_plot(lda_cv_preds, 'LDA')

# get threshold selection info
lda_thresh_perf <- probably::threshold_perf(lda_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
lda_max_sens <- lda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
lda_max_j <- lda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
lda_tmetrics_plot <- threshold_metric_plot(lda_thresh_perf, lda_max_sens, lda_max_j)

# get best threshold
lda_threshold <- as.numeric(lda_max_sens[1, 1])

# get predictions and metrics based on chosen threshold
lda_threshold_preds <- threshold_preds(lda_cv_preds, lda_threshold)

lda_threshold_metrics <- threshold_metrics(lda_threshold_preds, truth = Class, estimate = .pred_class)


# get confusion matrix for chosen threshold
lda_train_cm <- conf_mat(lda_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
lda_fit <- fit(lda_final, data)

# get predictions and metrics for holdout data
lda_test_preds <- threshold_preds(augment(lda_fit, new_data = holdout),
                                  lda_threshold)
lda_test_roc <- roc_plot(lda_test_preds, 'LDA')

lda_test_metrics <- test_metrics(lda_test_preds)

lda_test_cm <- conf_mat(lda_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r qda}
# define model spec
qda_spec <- discrim_quad(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
qda_final <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(qda_spec)


# fit resamples
qda_fitcv <- fit_resamples(qda_final, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
qda_cv_preds <-  collect_predictions(qda_fitcv)
qda_cv_metrics <- collect_metrics(qda_fitcv)

# get ROC plot for cross-validation
qda_cv_roc <- roc_plot(qda_cv_preds, 'QDA')

# get threshold selection info
qda_thresh_perf <- probably::threshold_perf(qda_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
qda_max_sens <- qda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
qda_max_j <- qda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
qda_tmetrics_plot <- threshold_metric_plot(qda_thresh_perf, qda_max_sens, qda_max_j)

# get best threshold
qda_threshold <- as.numeric( (qda_max_sens[1, 1] + qda_max_j[1, 1]) / 2 )

# get predictions and metrics based on chosen threshold
qda_threshold_preds <- threshold_preds(qda_cv_preds, qda_threshold)

qda_threshold_metrics <- threshold_metrics(qda_threshold_preds, truth = Class, estimate = .pred_class)


# get confusion matrix for chosen threshold
qda_train_cm <- conf_mat(qda_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
qda_fit <- fit(qda_final, data)

# get predictions and metrics for holdout data
qda_test_preds <- threshold_preds(augment(qda_fit, new_data = holdout),
                                  qda_threshold)
qda_test_roc <- roc_plot(qda_test_preds, 'QDA')

qda_test_metrics <- test_metrics(qda_test_preds)

qda_test_cm <- conf_mat(qda_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r logistic_regression}
log_spec <- logistic_reg(mode = 'classification',
                            engine = 'glm')
                            
# define workflow
log_final <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(log_spec)


# fit resamples
log_fitcv <- fit_resamples(log_final, resamples, control=cv_control)

# collect predictions and roc_auc from cross-validated fit
log_cv_preds <- collect_predictions(log_fitcv)
log_cv_metrics <- collect_metrics(log_fitcv)

# get ROC plot for cross-validation
log_cv_roc <- roc_plot(log_cv_preds, 'Logistic Regression')

# get threshold selection info
log_thresh_perf <- probably::threshold_perf(log_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
log_max_sens <- log_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
log_max_j <- log_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
log_tmetrics_plot <- threshold_metric_plot(log_thresh_perf, log_max_sens, log_max_j)

# get best threshold
log_threshold <- as.numeric(log_max_j[1, 1])

# get predictions and metrics based on chosen threshold
log_threshold_preds <- threshold_preds(log_cv_preds, log_threshold)

log_threshold_metrics <- threshold_metrics(log_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
log_train_cm <- conf_mat(log_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
log_fit <- fit(log_final, data)

# get predictions and metrics for holdout data
log_test_preds <- threshold_preds(augment(log_fit, new_data = holdout),
                                  log_threshold)
log_test_roc <- roc_plot(log_test_preds, 'Logistic Regression')

log_test_metrics <- test_metrics(log_test_preds)

log_test_cm <- conf_mat(log_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r penalized_logistic}
# define model spec
pen_spec <- logistic_reg(mode = 'classification',
                            engine = 'glmnet',
                            penalty = tune(),
                            mixture = tune())
                            
# define workflow
pen_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(pen_spec)

# set tuning parameters
pen_params <- extract_parameter_set_dials(pen_wf) %>% 
  update(
    penalty=penalty(c(-20,-1)),
    mixture=mixture(c(0,1))
  )

# tune with grid (or fit resamples)
pen_tune <- tune_grid(pen_wf,
                      resamples=resamples,
                      control=cv_control,
                      grid=grid_latin_hypercube(pen_params, size=50))

# get tuning results visualization
pen_tune_vis <- autoplot(pen_tune, metric = "roc_auc")

# finalize workflow and fit resamples with best parameters
pen_best_params <- select_best(pen_tune, metric="roc_auc")

pen_final <- finalize_workflow(pen_wf, pen_best_params) 

pen_fitcv <- fit_resamples(pen_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
pen_cv_preds <-  collect_predictions(pen_fitcv)
pen_cv_metrics <- collect_metrics(pen_fitcv)

# get ROC plot for cross-validation
pen_cv_roc <- roc_plot(pen_cv_preds, 'Penalized LR')

# get threshold selection info
pen_thresh_perf <- probably::threshold_perf(pen_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
pen_max_sens <- pen_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
pen_max_j <- pen_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
pen_tmetrics_plot <- threshold_metric_plot(pen_thresh_perf, pen_max_sens, pen_max_j)

# get best threshold
pen_threshold <- as.numeric(pen_max_j[1, 1])

# get predictions and metrics based on chosen threshold
pen_threshold_preds <- threshold_preds(pen_cv_preds, pen_threshold)

pen_threshold_metrics <- threshold_metrics(pen_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
pen_train_cm <- conf_mat(pen_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
pen_fit <- fit(pen_final, data)

# get predictions and metrics for holdout data
pen_test_preds <- threshold_preds(augment(pen_fit, new_data = holdout),
                                  pen_threshold)
pen_test_roc <- roc_plot(pen_test_preds, 'Penalized Log. Regression')

pen_test_metrics <- test_metrics(pen_test_preds)

pen_test_cm <- conf_mat(pen_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r ensemble - boost}
# define model spec

ens_spec <- boost_tree(mode="classification",trees = 10, tree_depth = tune(), learn_rate = tune()) %>%
    set_engine("xgboost")

# define workflow
ens_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(ens_spec)

# set tuning parameters
ens_params <- extract_parameter_set_dials(ens_wf)

# tune with grid (or fit resamples)
ens_tune <- tune_grid(ens_wf,
                      resamples = resamples,
                      grid = grid_regular(ens_params, levels = 10),
                      control = cv_control
                      )

# get tuning results visualization
ens_tune_vis <- autoplot(ens_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
ens_best_params <- select_best(ens_tune, metric = 'roc_auc')

ens_final <- ens_wf %>% 
  finalize_workflow(ens_best_params)

ens_fitcv <- fit_resamples(ens_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
ens_cv_preds <-  collect_predictions(ens_fitcv)
ens_cv_metrics <- collect_metrics(ens_fitcv)

# get ROC plot for cross-validation
ens_cv_roc <- roc_plot(ens_cv_preds, 'XGBoost Model')

# get threshold selection info
ens_thresh_perf <- probably::threshold_perf(ens_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
ens_max_sens <- ens_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
ens_max_j <- ens_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
ens_tmetrics_plot <- threshold_metric_plot(ens_thresh_perf, ens_max_sens, ens_max_j)


# get best threshold 
ens_threshold <- as.numeric(ens_max_j[1,1])

# get predictions and metrics based on chosen threshold
ens_threshold_preds <- threshold_preds(ens_cv_preds, ens_threshold)

ens_threshold_metrics <- threshold_metrics(ens_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
ens_train_train_cm <- conf_mat(ens_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##

# fit final workflow to training data
ens_fit <- fit(ens_final,data)

# get predictions and metrics for holdout data
ens_test_preds <- threshold_preds(augment(ens_fit, new_data = holdout),
                                  ens_threshold)
ens_test_roc <- roc_plot(ens_test_preds, "XGBoost Model")

ens_test_metrics <- test_metrics(ens_test_preds)

ens_test_cm <- conf_mat(ens_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r}
## PART I ##
# define model spec
svml_spec <- svm_linear(mode = "classification", engine = "kernlab", cost = tune(),
                        margin = tune()) 

# define workflow
svml_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(svml_spec)

# set tuning parameters
svml_params <- extract_parameter_set_dials(svml_wf)

# tune with grid (or fit resamples)
svml_tune <- tune_bayes(svml_wf,
                      resamples = resamples,
                      metrics = metric_set(roc_auc),
                      param_info = svml_params, iter = 20
                      )

# get tuning results visualization
svml_tune_vis <- autoplot(svml_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
svml_best_params <- select_best(svml_tune, metric = 'roc_auc')

svml_final <- svml_wf %>% 
  finalize_workflow(svml_best_params)

svml_fitcv <- fit_resamples(svml_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
svml_cv_preds <-  collect_predictions(svml_fitcv)
svml_cv_metrics <- collect_metrics(svml_fitcv)

# get ROC plot for cross-validation
svml_cv_roc <- roc_plot(svml_cv_preds, 'Linear SVM')

# get threshold selection info
svml_thresh_perf <- probably::threshold_perf(svml_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
svml_max_sens <- svml_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
svml_max_j <- svml_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
svml_tmetrics_plot <- threshold_metric_plot(svml_thresh_perf, svml_max_sens, svml_max_j)

# get best threshold
svml_threshold <- as.numeric(svml_max_j[1,1])

# get predictions and metrics based on chosen threshold
svml_threshold_preds <- threshold_preds(svml_cv_preds, svml_threshold)

svml_threshold_metrics <- threshold_metrics(svml_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
svml_train_train_cm <- conf_mat(svml_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##
# fit final workflow to training data
svml_fit <- fit(svml_final, data)

# get predictions and metrics for holdout data
svml_test_preds <- threshold_preds(augment(svml_fit, new_data = holdout),
                                  svml_threshold)
svml_test_roc <- roc_plot(svml_test_preds, "Linear SVM")

svml_test_metrics <- test_metrics(svml_test_preds)

svml_test_cm <- conf_mat(svml_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r}
## PART I ##
# define model spec
svmp_spec <- svm_poly(mode = "classification", engine = "kernlab", cost = tune(),
                        margin = tune(), degree = tune()) 

# define workflow
svmp_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(svmp_spec)

# set tuning parameters
svmp_params <- extract_parameter_set_dials(svmp_wf)

# tune with grid (or fit resamples)
svmp_tune <- tune_bayes(svmp_wf,
                      resamples = resamples,
                      metrics = metric_set(roc_auc),
                      param_info = svmp_params, iter = 20
                      )

# get tuning results visualization
svmp_tune_vis <- autoplot(svmp_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
svmp_best_params <- select_best(svmp_tune, metric = 'roc_auc')

svmp_final <- svmp_wf %>% 
  finalize_workflow(svmp_best_params)

svmp_fitcv <- fit_resamples(svmp_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
svmp_cv_preds <-  collect_predictions(svmp_fitcv)
svmp_cv_metrics <- collect_metrics(svmp_fitcv)

# get ROC plot for cross-validation
svmp_cv_roc <- roc_plot(svmp_cv_preds, 'Polynomial SVM')

# get threshold selection info
svmp_thresh_perf <- probably::threshold_perf(svmp_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
svmp_max_sens <- svmp_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
svmp_max_j <- svmp_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
svmp_tmetrics_plot <- threshold_metric_plot(svmp_thresh_perf, svmp_max_sens, svmp_max_j)

# get best threshold
svmp_threshold <- as.numeric(svmp_max_j[1,1])

# get predictions and metrics based on chosen threshold
svmp_threshold_preds <- threshold_preds(svmp_cv_preds, svmp_threshold)

svmp_threshold_metrics <- threshold_metrics(svmp_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
svmp_train_train_cm <- conf_mat(svmp_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##
# fit final workflow to training data
svmp_fit <- fit(svmp_final, data)

# get predictions and metrics for holdout data
svmp_test_preds <- threshold_preds(augment(svmp_fit, new_data = holdout),
                                  svmp_threshold)
svmp_test_roc <- roc_plot(svmp_test_preds, "Polynomial SVM")

svmp_test_metrics <- test_metrics(svmp_test_preds)

svmp_test_cm <- conf_mat(svmp_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r}
## PART I ##
# define model spec
svmr_spec <- svm_rbf(mode = "classification", engine = "kernlab", cost = tune(),
                        margin = tune(), rbf_sigma = tune()) 

# define workflow
svmr_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(svmr_spec)

# set tuning parameters
svmr_params <- extract_parameter_set_dials(svmr_wf)


# tune with grid (or fit resamples)
svmr_tune <- tune_grid(svmr_wf,
                      resamples = resamples,
                      grid = grid_random(svmr_params, size = 10),
                      control = cv_control
                      )

# get tuning results visualization
svmr_tune_vis <- autoplot(svmr_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
svmr_best_params <- select_best(svmr_tune, metric = 'roc_auc')

svmr_final <- svmr_wf %>% 
  finalize_workflow(svmr_best_params)

svmr_fitcv <- fit_resamples(svmr_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
svmr_cv_preds <-  collect_predictions(svmr_fitcv)
svmr_cv_metrics <- collect_metrics(svmr_fitcv)

# get ROC plot for cross-validation
svmr_cv_roc <- roc_plot(svmr_cv_preds, 'Radial SVM')

# get threshold selection info
svmr_thresh_perf <- probably::threshold_perf(svmr_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
svmr_max_sens <- svmr_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
svmr_max_j <- svmr_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
svmr_tmetrics_plot <- threshold_metric_plot(svmr_thresh_perf, svmr_max_sens, svmr_max_j)

# get best threshold
svmr_threshold <- as.numeric(svmr_max_j[1,1])

# get predictions and metrics based on chosen threshold
svmr_threshold_preds <- threshold_preds(svmr_cv_preds, svmr_threshold)

svmr_threshold_metrics <- threshold_metrics(svmr_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
svmr_train_train_cm <- conf_mat(svmr_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##
# fit final workflow to training data
svmr_fit <- fit(svmr_final, data)

# get predictions and metrics for holdout data
svmr_test_preds <- threshold_preds(augment(svmr_fit, new_data = holdout),
                                  svmr_threshold)
svmr_test_roc <- roc_plot(svmr_test_preds, "Radial SVM")

svmr_test_metrics <- test_metrics(svmr_test_preds)

svmr_test_cm <- conf_mat(svmr_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r compare-models}
# get all cross-validation plots and metrics
cv_rocs <- ( log_cv_roc + pen_cv_roc  + knn_cv_roc) /
  ( lda_cv_roc + qda_cv_roc + ens_cv_roc ) /
  ( svml_cv_roc + svmp_cv_roc + svmr_cv_roc)

# precision calculations for threshold metrics
knnp1 <- 1999/(1999+291) #KNN
ldap1 <- 1791/(1791+1284) #LDA
qdap1 <- 2017/(2017+1732) #QDA
logp1 <- 1974/(1974+786) #Log
penp1 <- 1990/(1990+961) #Pen
ensp1 <- 2017/(2017+626) #Ens
svmlp1 <- 1968/(1968+851) #SVML
svmpp1 <- 2018/(2018+635) #SVMP
svmrp1 <- 2014/(2014+781) #SVMR

# precision calculations for holdout performance
knnp2 <- 13452/(13452+31424) #KNN
ldap2 <- 13860/(13860+63633) #LDA
qdap2 <- 13586/(13586+71822) #QDA
logp2 <- 14479/(14479+193475) #Log
penp2 <- 14480/(14480+206144) #Pen
ensp2 <- 13863/(13863+64505) #Ens
svmlp2 <-  14478/(14478+204855) #SVML
svmpp2 <- 14480/(14480+101993) #SVMP
svmrp2 <- 14480/(14480+170042) #SVMR

# We ran into an issue where our metrics were being calculated based on
# Non-Tarp as our target class. For this reason the sensitivity and specificity metrics were switched, and
# precision was calculated separately below

compare_cv_metrics <- bind_rows(
  knn_cv_metrics %>% mutate(model = 'KNN'),
  lda_cv_metrics %>% mutate(model = 'LDA'),
  qda_cv_metrics %>% mutate(model = 'QDA'),
  log_cv_metrics %>% mutate(model = 'logreg'),
  pen_cv_metrics %>% mutate(model = 'penalized log'),
  ens_cv_metrics %>% mutate(model = 'XGBoost'),
  svml_cv_metrics %>% mutate(model = 'linear SVM'),
  svmp_cv_metrics %>% mutate(model = 'polynomial SVM'),
  svmr_cv_metrics %>% mutate(model = 'radial SVM')
) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = mean) %>% 
  dplyr::select(model, roc_auc)

compare_threshold_metrics <- bind_rows(
  knn_threshold_metrics %>% mutate(model = 'KNN'),
  lda_threshold_metrics %>% mutate(model = 'LDA'),
  qda_threshold_metrics %>% mutate(model = 'QDA'),
  log_threshold_metrics %>% mutate(model = 'logreg'),
  pen_threshold_metrics %>% mutate(model = 'penalized log'),
  ens_threshold_metrics %>% mutate(model = 'XGBoost'),
  svml_threshold_metrics %>% mutate(model = 'linear SVM'),
  svmp_threshold_metrics %>% mutate(model = 'polynomial SVM'),
  svmr_threshold_metrics %>% mutate(model = 'radial SVM')
) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = .estimate) %>% 
  mutate(threshold = c(knn_threshold, lda_threshold, qda_threshold, log_threshold,
                       pen_threshold, ens_threshold, svml_threshold, svmp_threshold,
                       svmr_threshold)) %>% 
  rename(sensitivity = "specificity",
         specificity = "sensitivity") %>% 
  mutate(precision = c(knnp1, ldap1, qdap1, logp1, penp1, ensp1, svmlp1, svmpp1, svmrp1))


  

# get all metrics and plots on the holdout dataset
test_rocs <- ( log_test_roc + pen_test_roc  + knn_test_roc) /
  ( lda_test_roc + qda_test_roc + ens_test_roc ) /
  ( svml_test_roc + svmp_test_roc + svmr_test_roc )

compare_test_metrics <- bind_rows(
  knn_test_metrics %>% mutate(model = 'KNN'),
  lda_test_metrics %>% mutate(model = 'LDA'),
  qda_test_metrics %>% mutate(model = 'QDA'),
  log_test_metrics %>% mutate(model = 'logreg'),
  pen_test_metrics %>% mutate(model = 'penalized log'),
  ens_test_metrics %>% mutate(model = 'XGBoost'),
  svml_test_metrics %>% mutate(model = 'linear SVM'),
  svmp_test_metrics %>% mutate(model = 'polynomial SVM'),
  svmr_test_metrics %>% mutate(model = 'radial SVM')
) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = .estimate) %>% 
  rename(sensitivity = "specificity", # Renaming due to error with how R is calculating levels
         specificity = "sensitivity") %>% 
  mutate(precision = c(knnp2, ldap2, qdap2, logp2, penp2, ensp2, svmlp2, svmpp2, svmrp2))

# create point plot to check for overfitting based on ROC AUC and j-index
overfit_plot <- bind_rows(
  compare_cv_metrics %>% 
    dplyr::select(model, roc_auc) %>% 
    mutate(data = 'cv'),
  compare_threshold_metrics %>% 
    dplyr::select(model, j_index) %>% 
    mutate(data = 'train'),
  compare_test_metrics %>% 
    dplyr::select(model, roc_auc, j_index) %>% 
    mutate(data = 'test')
) %>% 
  pivot_longer(cols = c(roc_auc, j_index)) %>% 
  mutate(data = factor(data, levels = c('cv', 'train', 'test'))) %>% 
  ggplot(aes(x = model, y = value, color = data)) +
  geom_point(size = 2) +
  facet_wrap(~name, scales = 'free') +
  labs(x = '', y = 'metric value') +
  theme(axis.text.x = element_text(angle = 30))

```

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
stopCluster(cl)
registerDoSEQ()
```

# Figures

```{r EDA_Boxplots}
#| fig.width: 6
#| fig.height: 7
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the training data pixel values broken out by each colour and class.
#| dev: "png"
#| dpi: 100
g1_box <- ggplot(data, aes(x=Class, y=Red))+
  geom_boxplot(fill="#F8766D")+
  labs(title= "Boxplots of Red Pixel Value by Class")

g2_box <- ggplot(data, aes(x=Class, y=Green))+
  geom_boxplot(fill="#00BA38")+
  labs(title= "Boxplots of Green Pixel Value by Class")

g3_box <- ggplot(data, aes(x=Class, y=Blue))+
  geom_boxplot(fill="#619CFF")+
  labs(title= "Boxplots of Blue Pixel Value by Class")

g1_box/g2_box/g3_box

```

```{r EDA_Countplot_Class_ungrouped }
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of total counts for the training data broken out by each class
#| dev: "png"
#| dpi: 100
ggplot(data, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Class", x="Count")
```

```{r}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of Classes Blue-Tarp and Non-Tarp for the training data
#| dev: "png"
#| dpi: 100
data <- data %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
ggplot(data, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Classes", x="Count")
```

```{r EDA_Count_Table_1}
table(data$Class) %>% knitr::kable(caption = "Count of Class")
```

```{r EDA_boxplot-binary}
#| fig.width: 20
#| fig.height: 8
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the training data pixel values broken out by each color and class.
#| dev: "png"
#| dpi: 200
bluedata <- data[data$Class == 'Tarp', ]
otherdata <- data[data$Class == 'Non-Tarp',]
g1_tarpden <- ggplot(bluedata,aes(x=Red))+
  geom_boxplot(fill="#F8766D")+
  labs(title= "Box plot of Red Pixel Value of Blue-tarps")
g1_nontarpden <- ggplot(otherdata,aes(x=Red))+
  geom_boxplot(fill="#F8766D")+
  labs(title= "Box plot of Red Pixel Value of Non-tarps")
g2_tarpden <- ggplot(bluedata,aes(x=Green))+
  geom_boxplot(fill="#00BA38")+
  labs(title= "Box plot of Green Pixel Value of Blue-tarps")
g2_nontarpden <- ggplot(otherdata,aes(x=Green))+
  geom_boxplot(fill="#00BA38")+
  labs(title= "Box plot of Green Pixel Value of Non-tarps")
g3_tarpden <- ggplot(bluedata,aes(x=Blue))+
  geom_boxplot(fill="#619CFF")+
  labs(title= "Box plot of Blue Pixel Value of Blue-tarps")
g3_nontarpden <- ggplot(otherdata,aes(x=Blue))+
  geom_boxplot(fill="#619CFF")+
  labs(title= "Box plot of Blue Pixel Value of Non-tarps")

(g1_tarpden+g1_nontarpden)/(g2_tarpden+g2_nontarpden)/(g3_tarpden+g3_nontarpden)
```

```{r EDA_boxplot_binaryfull}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of Classes Blue-Tarp and Non-Tarp for the hold-out set.
#| dev: "png"
#| dpi: 100
ggplot(data_full, aes(Class), fill = Class)+
  geom_bar()+
  labs(title= "Count Plot of Classes for Holdout Set", x="Count")
```

```{r}
table(data_full$Class) %>% knitr::kable(caption = "Count of Class for holdout set")
```

```{r density full data}
#| fig.width: 20
#| fig.height: 8
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each color and class for the hold-out set.
#| dev: "png"
#| dpi: 200
bluedata_full <- data_full[data_full$Class == 'Tarp', ]
otherdata_full<- data_full[data_full$Class == 'Non-Tarp',]
g1_tarpden_full <- ggplot(bluedata_full,aes(x=Red))+
  geom_boxplot(fill="#F8766D")+
  labs(title= "Box plot of Red Pixel Value of Blue-tarps")
g1_nontarpden_full <- ggplot(otherdata_full,aes(x=Red))+
  geom_boxplot(fill="#F8766D")+
  labs(title= "Box plot of Red Pixel Value of Non-tarps")
g2_tarpden_full <- ggplot(bluedata_full,aes(x=Green))+
  geom_boxplot(fill="#00BA38")+
  labs(title= "Box plot of Green Pixel Value of Blue-tarps")
g2_nontarpden_full <- ggplot(otherdata_full,aes(x=Green))+
  geom_boxplot(fill="#00BA38")+
  labs(title= "Box plot of Green Pixel Value of Non-tarps")
g3_tarpden_full <- ggplot(bluedata_full,aes(x=Blue))+
  geom_boxplot(fill="#619CFF")+
  labs(title= "Box plot of Blue Pixel Value of Blue-tarps")
g3_nontarpden_full <- ggplot(otherdata_full,aes(x=Blue))+
  geom_boxplot(fill="#619CFF")+
  labs(title= "Box plot of Blue Pixel Value of Non-tarps")

(g1_tarpden_full+g1_nontarpden_full)/(g2_tarpden_full+g2_nontarpden_full)/(g3_tarpden_full+g3_nontarpden_full)
```

```{r figure-X-knn-tune}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC AUC based on number of principal components and nearest neighbors.
#| dev: "png"
#| dpi: 100
knn_tune_vis
```

```{r figure-X-knn-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the KNN model with k = 40 nearest neighbors. Vertical lines show maximum sensitivity between 0.01 and 0.09, and maximum j-index at 0.09.
#| dev: "png"
#| dpi: 100
knn_tmetrics_plot
```

```{r knn-conf-mat}
knn_train_cm
```

```{r}
knn_test_cm
```

```{r figure-X-lda-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Four metrics based on threshold value for the LDA model. Vertical lines show maximum j-index and sensitivity at 0.01.
#| dev: "png"
#| dpi: 100
lda_tmetrics_plot
```
 
```{r lda-cm}
lda_train_cm
```

```{r}
lda_test_cm
```

```{r figure-X-qda-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Four metrics based on threshold value for the QDA model. Vertical lines show maximum j-index and sensitivity at 0.02 and 0.01, respectively.
#| dev: "png"
#| dpi: 100
qda_tmetrics_plot
```

```{r qda-cm}
qda_train_cm
```

```{r}
qda_test_cm
```

```{r figure-X-pen-tune}
#| fig.width: 6
#| fig.height: 3
#| fig.align: center
#| out.width: 100%
#| fig.cap: ROC based on penalty and mixture values. Left hand chart corresponds to penalty value, right hand chart corresponds to mixture.
#| dev: "png"
#| dpi: 100
pen_tune_vis
```

```{r figure-X-log-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Logistic Regression model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.05.
#| dev: "png"
#| dpi: 100
log_tmetrics_plot
```

```{r figure-X-pen-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Penalized Logistic Regression model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.04.
#| dev: "png"
#| dpi: 100
pen_tmetrics_plot
```

```{r log-conf-mat}
log_train_cm
```

```{r pen-conf-mat}
pen_train_cm
```

```{r}
log_test_cm
```

```{r}
pen_test_cm
```

```{r figure-X-ens-tune}
#| fig.width: 6
#| fig.height: 3
#| fig.align: center
#| out.width: 100%
#| fig.cap: ROC values of the Boosted Gradient model that considers different learning rates along the tree depth
#| dev: "png"
#| dpi: 100
ens_tune_vis
```

```{r figure-X-ens-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the XGBoost Model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.04.
#| dev: "png"
#| dpi: 100
ens_tmetrics_plot
```

```{r}
ens_train_train_cm
```

```{r}
ens_test_cm
```

```{r figure-X-svml-tune}
#| fig.width: 6
#| fig.height: 3
#| fig.align: center
#| out.width: 100%
#| fig.cap: Cost value for the SVM to the left that dictates cost for values outside of margin and insensitivty margin dictates tolerance levels of margin
#| dev: "png"
#| dpi: 100
svml_tune_vis
```

```{r figure-X-svmp-tune}
#| fig.width: 6
#| fig.height: 3
#| fig.align: center
#| out.width: 100%
#| fig.cap: Cost value for the SVM to the left that dictates cost for values outside of margin, degree of interaction refers to the degree of the interaction term, insensitivty margin dictates tolerance levels of margin
#| dev: "png"
#| dpi: 100
svmp_tune_vis
```

```{r figure-X-svmr-tune}
#| fig.width: 6
#| fig.height: 3
#| fig.align: center
#| out.width: 100%
#| fig.cap: Cost value for the SVM to the left that dictates cost for values outside of margin,insensitivty margin dictates tolerance levels of margin, sigma represents how flexible the decision boundary is with larger numbers being more biased
#| dev: "png"
#| dpi: 100
svmr_tune_vis
```

```{r figure-X-svml-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Linear SVM Model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.04.
#| dev: "png"
#| dpi: 100
svml_tmetrics_plot
```

```{r figure-X-svmp-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Polynomial SVM Model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.04.
#| dev: "png"
#| dpi: 100
svmp_tmetrics_plot
```

```{r figure-X-svmr-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Radial SVM Model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.04.
#| dev: "png"
#| dpi: 100
svmr_tmetrics_plot
```

```{r}
svml_train_train_cm
```

```{r}
svml_test_cm
```

```{r}
svmp_train_train_cm
```

```{r}
svmp_test_cm
```

```{r}
svmr_train_train_cm
```

```{r}
svmr_test_cm
```

```{r figure-X-ROC-metrics}
#| fig.align: center
#| fig.width: 18
#| fig.height: 18
#| out.width: 100%
#| fig.cap: CV ROC plots for all models
#| dpi: 100
cv_rocs
```

```{r table-2-metrics}
compare_cv_metrics %>% knitr::kable(caption = "Comparison of cross-validated AUC for all models", digits = 4)
```

```{r table-3-metrics}
compare_threshold_metrics %>% knitr::kable(caption = "Comparison of Threshold Metrics for all models", digits = 4)
```

```{r table-4-metrics}
compare_test_metrics %>% knitr::kable(caption = "Comparison of Metrics on Holdout Dataset for all models", digits = 4)
```

```{r figure-X-ROC-test-metrics}
#| fig.align: center
#| fig.width: 18
#| fig.height: 18
#| out.width: 100%
#| fig.cap: Holdout ROC plots for all models
#| dpi: 100
test_rocs
```

```{r figure-X-overfit-plots}
#| fig.align: center
#| fig.width: 10
#| fig.height: 5
#| out.width: 70%
#| fig.cap: Plot showing J Index and ROC AUC Values of all plots to compare for potential overfitting
#| dev: "png"
#| dpi: 100
overfit_plot
```

