Outline Disaster Relief project



## 1.1 Introduction

The aim of the experiment is to build a model that can accurately identify blue tarps from pictures. The purpose is to help displaced people in Haiti who are known to be sheltering under blue tarps. Aircraft have flown over Haiti and taken pictures of the land, but there's no way humans could parse through all of the pictures quickly enough to get help to all the people in need.


## 1.2 Background and methods

We will investigate five types of models: logistic regression, linear determinant analysis (LDA), quadratic determinant analysis (QDA), k-nearest neighbors (KNN), and penalized logistic regression. For each model, we will thoroughly explain parameter tuning, model validation, threshold selection, and metrics used for performance evaluaiton. Sensitivity will be espeically important, because we want to make sure we get all the blue tarps, even if it means we have some false positives. 

We will use 10-fold cross validation to validate and compare the models to help us choose the best-performing one, because it is more robust than a simple train-holdout split yet not so computationally intensive as leave-one-out cross validation (LOOCV). We choose 10-fold cross-validation instead of the bootstrapping method because it is more useful for comparing multiple models efficiently vs looking at a single sample statistic at a time. We may use bootstrapping to validate our chosen model after using cross-validation to compare them.

## 1.3 Results

The results will be the metrics of each model, including accuracy, specificity, sensitivity, precision, j-index, AUC, and possibly others. In this section, we will thoroughly explore and explain parameter tuning (e.g. k for the KNN model), including threshold selection, using visualizations such as tables and plots. We will use this section to discuss the performance of each model individually, including explorint ROC and UC for each model. 

## 1.4 Discussion

In this section, we will take the exploration from the results section and put it all together to compare the models. We will address why sensitivity is a more important metric than perhaps accuracy or mone other metric. We will discuss the implications of the results, including which ones may be overfitted and which one may be better suited for our purpose, and why by addressing the bias-variance trade-off.

## 1.5 Conclusion

We will state our conclusion about which algorithm works best, including a thorough justification based on the needs of the problem.

We will state two more conclusions related to the implications of our results and possible next steps or what could be done to improve results. We have not yet decided on our other two conclusions, and will need to work with the data to see what catches our interest.


----------------------------------------------------------------------------------

Comments from Prof. Gedeck:

## Introduction:
- What is the key idea for locating people and why do we expect it to work?

## Background and methods:
- You will need to do some EDA
- EDA - focus your discussion of the results on what it means for the model
- Be selective with your EDA. Focus on results that have a relevance to the project. You can always mention that you tried other visualizations, but didn’t get much more out of it. If you add a graph, it should add to your analysis and cover something that cannot already be seen in other graphs.
- Model training consists of these steps:
- Model validation using 10-fold cross validation; this includes model tuning if applicable
- what metric will you use?
- Threshold selection
- what metric will you use?
- Are “penalized logistic regression” and “logistic regression” really two different models or just variations of the same?

## Results:
- When you present metrics, talk about their relevance for the objective of this project
- When you present your results, split it into results from model validation and results after threshold selection. e.g. ROC curves are only relevant for discussing model validation results and not threshold selection; reporting accuracy or any other threshold dependent metric prior to threshold selection is irrelevant.
- If you visualize the same analysis for each model, combine them either in a single graph or a combination of graphs in one figure (using patchwork)

## Discussion:
- Focus your discussion also on differences between models.
- For example LDA and QDA are conceptually similar but differ in the way they can represent the decision boundary. 
- What do the differences in the results for LDA and QDA tell you?
- Another example that you can discuss is the difference between , penalized logistic regression and logistic regression.
