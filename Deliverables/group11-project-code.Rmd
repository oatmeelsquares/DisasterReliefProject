---
title: 'Disaster Relief Project, Part 1'
author: "Becky Desrosiers, Abner Casillas-Colon, Rachel Daniel"
date: "2024-04-22"
---

```{r r-setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      autodep = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE
                      )
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(discrim)
library(patchwork)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```


```{r load-data, include = FALSE}
# load data from file
file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
#file = '../data.csv'
data <- read_csv(file) #%>% 
  #mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
```


```{r EDA_Boxplots}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each colour and class.
#| dev: "png"
#| dpi: 100
g1_box <- ggplot(data, aes(x=Class, y=Red))+
  geom_boxplot()+
  labs(title= "Boxplots of Red Pixel Value by Class")

g2_box <- ggplot(data, aes(x=Class, y=Green))+
  geom_boxplot()+
  labs(title= "Boxplots of Green Pixel Value by Class")

g3_box <- ggplot(data, aes(x=Class, y=Blue))+
  geom_boxplot()+
  labs(title= "Boxplots of Blue Pixel Value by Class")

g1_box/g2_box/g3_box

```

```{r EDA_Countplot_Class_ungrouped }
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of total counts broken out by each class
#| dev: "png"
#| dpi: 100
ggplot(data, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Class", x="Count")
```

```{r}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of Classes: Blue-Tarp and Non-Tarp
#| dev: "png"
#| dpi: 100
data <- data %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
ggplot(data, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Classes", x="Count")
```


```{r EDA_Count_Table_1}
table(data$Class) %>% knitr::kable(caption = "Count of Class")
```


```{r EDA_boxplot-binary}
#| fig.width: 20
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each color and class.
#| dev: "png"
#| dpi: 200
bluedata <- data[data$Class == 'Tarp', ]
otherdata <- data[data$Class == 'Non-Tarp',]
g1_tarpden <- ggplot(bluedata,aes(x=Red))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Blue-tarps")
g1_nontarpden <- ggplot(otherdata,aes(x=Red))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Non-tarps")
g2_tarpden <- ggplot(bluedata,aes(x=Green))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Blue-tarps")
g2_nontarpden <- ggplot(otherdata,aes(x=Green))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Non-tarps")
g3_tarpden <- ggplot(bluedata,aes(x=Blue))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Blue-tarps")
g3_nontarpden <- ggplot(otherdata,aes(x=Blue))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Non-tarps")

(g1_tarpden+g1_nontarpden)/(g2_tarpden+g2_nontarpden)/(g3_tarpden+g3_nontarpden)
```

```{r holdout processing EDA}

columns = c('ID', 'X','Y','Map X','Map Y','Lat','Lon','B1','B2','B3')

data_67_BT <- read_table("orthovnir067_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_57_NON <- read_table("orthovnir057_ROI_NON_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_67_NOT <- read_table("orthovnir067_ROI_NOT_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_69_NOT <- read_table("orthovnir069_ROI_NOT_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_69_bt <- read_table("orthovnir069_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_78_bt <- read_table("orthovnir078_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_78_NON <- read_table("orthovnir078_ROI_NON_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))
  
```

```{r holdout EDA}
data_full <- bind_rows(
  data_67_BT,
  data_57_NON,
  data_67_NOT,
  data_69_NOT,
  data_69_bt,
  data_78_bt,
  data_78_NON)
```

```{r EDA_boxplot_binaryfull}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of Classes: Blue-Tarp and Non-Tarp
#| dev: "png"
#| dpi: 100
ggplot(data_full, aes(Class), fill = Class)+
  geom_bar()+
  labs(title= "Count Plot of Classes for Holdout Set", x="Count")
```


```{r}
table(data_full$Class) %>% knitr::kable(caption = "Count of Class for holdout set")
```


```{r density full data}
#| fig.width: 20
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each color and class for the hold-out set.
#| dev: "png"
#| dpi: 200
bluedata_full <- data_full[data_full$Class == 'Tarp', ]
otherdata_full<- data_full[data_full$Class == 'Non-Tarp',]
g1_tarpden_full <- ggplot(bluedata_full,aes(x=B1))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Blue-tarps")
g1_nontarpden_full <- ggplot(otherdata_full,aes(x=B1))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Non-tarps")
g2_tarpden_full <- ggplot(bluedata_full,aes(x=B2))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Blue-tarps")
g2_nontarpden_full <- ggplot(otherdata_full,aes(x=B2))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Non-tarps")
g3_tarpden_full <- ggplot(bluedata_full,aes(x=B3))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Blue-tarps")
g3_nontarpden_full <- ggplot(otherdata_full,aes(x=B3))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Non-tarps")

(g1_tarpden_full+g1_nontarpden_full)/(g2_tarpden_full+g2_nontarpden_full)/(g3_tarpden_full+g3_nontarpden_full)
```

```{r}
#| fig.width: 20
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each color and class for the training data.
#| dev: "png"
#| dpi: 200
(g1_tarpden+g1_nontarpden)/(g2_tarpden+g2_nontarpden)/(g3_tarpden+g3_nontarpden)
```

```{r convenience-functions}
# define functions to use later for convenience
cv_control <- control_resamples(save_pred = TRUE)

roc_plot <- function(model_preds, model_name) {
  roc <- model_preds %>% 
    roc_curve(truth = Class,
              .pred_Tarp,
              event_level = 'second') %>% 
    autoplot()
    zoom <- roc + coord_cartesian(xlim = c(0, 0.1), ylim = c(0.9, 1)) +
    theme(axis.title.x=element_blank(), #remove x axis labels
          axis.text.x = element_blank(),
          axis.title.y=element_blank()  #remove y axis labels
          )
  roc +
    labs(title = paste(model_name)) +
    inset_element(zoom, left = 0.4, right = 0.95, bottom = 0.05, top = 0.6)
}


threshold_metric_plot <- function(thresh_perf, max_sens, max_j) {
  ggplot(thresh_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
    geom_line() +
    geom_vline(data = max_sens, aes(xintercept = .threshold, color = .metric)) +
    geom_vline(data = max_j, aes(xintercept = .threshold, color = .metric)) +
    scale_x_continuous(breaks = seq(0, 1, 0.1)) +
    labs(x = 'Threshold', y = 'Metric value')
}

threshold_metrics <- metric_set(j_index,
                                sensitivity,
                                specificity,
                                accuracy,
                                precision)

test_metrics <- function(model_test_preds) {
  bind_rows(
    roc_auc(model_test_preds,
            truth = Class,
            .pred_Tarp,
            event_level = 'second'),
    threshold_metrics(model_test_preds,
                      truth = Class,
                      estimate = .pred_class,
                      )
  )
}


threshold_preds <- function(preds, threshold) {
    preds <- preds %>% 
    mutate(.pred_class = factor(ifelse(.pred_Tarp >= threshold, 'Tarp', 'Non-Tarp')))
}

```


```{r model-setup}
# prepare resamples for 10-fold cross-validation
set.seed(6030)
resamples <- vfold_cv(data, v = 10, strata=Class)

# define formula
formula <- Class ~ Red + Green + Blue

# define basic recipe
rec <- recipe(formula, data = data)
```


```{r knn}
## PART I ##
# define model spec
knn_spec <- nearest_neighbor(mode = 'classification',
                            engine = 'kknn',
                            neighbors = parsnip::tune())

# define workflow
knn_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(knn_spec)

# set tuning parameters
knn_params <- extract_parameter_set_dials(knn_wf) %>% 
  update(neighbors = neighbors(c(2, 100)))

# tune with grid (or fit resamples)
knn_tune <- tune_grid(knn_wf,
                      resamples = resamples,
                      grid = grid_regular(knn_params, levels = 50),
                      control = cv_control
                      )

# get tuning results visualization
knn_tune_vis <- autoplot(knn_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
knn_best_params <- select_best(knn_tune, metric = 'roc_auc')

knn_final <- knn_wf %>% 
  finalize_workflow(knn_best_params)

knn_fitcv <- fit_resamples(knn_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
knn_cv_preds <-  collect_predictions(knn_fitcv)
knn_cv_metrics <- collect_metrics(knn_fitcv)

# get ROC plot for cross-validation
knn_cv_roc <- roc_plot(knn_cv_preds, 'KNN')

# get threshold selection info
knn_thresh_perf <- probably::threshold_perf(knn_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
knn_max_sens <- knn_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
knn_max_j <- knn_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
knn_tmetrics_plot <- threshold_metric_plot(knn_thresh_perf, knn_max_sens, knn_max_j)

# get best threshold
knn_threshold <- as.numeric(knn_max_j[1, 1])

# get predictions and metrics based on chosen threshold
knn_threshold_preds <- threshold_preds(knn_cv_preds, knn_threshold)

knn_threshold_metrics <- threshold_metrics(knn_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
knn_train_cm <- conf_mat(knn_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##
# fit final workflow to training data
knn_fit <- fit(knn_final, data)

# get predictions and metrics for holdout data
knn_test_preds <- threshold_preds(augment(knn_fit, new_data = holdout),
                                  knn_threshold)
knn_test_roc <- roc_plot(knn_test_preds, 'KNN')

knn_test_metrics <- test_metrics(knn_test_preds)

knn_test_cm <- conf_mat(knn_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```




```{r figure-X-knn-tune}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC AUC based on number of principal components and nearest neighbors.
#| dev: "png"
#| dpi: 100
knn_tune_vis
```


```{r figure-X-knn-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the KNN model with k = 40 nearest neighbors. Vertical lines show maximum sensitivity between 0.01 and 0.09, and maximum j-index at 0.09.
#| dev: "png"
#| dpi: 100
knn_tmetrics_plot
```


```{r knn-conf-mat}
knn_train_cm
```


```{r}
knn_test_cm
```

```{r lda}
# define model spec
lda_spec <- discrim_linear(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
lda_final <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(lda_spec)

# fit resamples
lda_fitcv <- fit_resamples(lda_final, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
lda_cv_preds <-  collect_predictions(lda_fitcv)
lda_cv_metrics <- collect_metrics(lda_fitcv)

lda_cv_roc <- roc_plot(lda_cv_preds, 'LDA')

# get threshold selection info
lda_thresh_perf <- probably::threshold_perf(lda_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
lda_max_sens <- lda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
lda_max_j <- lda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
lda_tmetrics_plot <- threshold_metric_plot(lda_thresh_perf, lda_max_sens, lda_max_j)

# get best threshold
lda_threshold <- as.numeric(lda_max_sens[1, 1])

# get predictions and metrics based on chosen threshold
lda_threshold_preds <- lda_cv_preds %>% 
 mutate(.pred_class = factor(ifelse(.pred_Tarp >= lda_threshold, 'Tarp', 'Non-Tarp')))

lda_threshold_metrics <- threshold_metrics(lda_threshold_preds, truth = Class, estimate = .pred_class)


# get confusion matrix for chosen threshold
lda_train_cm <- conf_mat(lda_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
lda_fit <- fit(lda_final, data)

# get predictions and metrics for holdout data
lda_test_preds <- threshold_preds(augment(lda_fit, new_data = holdout),
                                  lda_threshold)
lda_test_roc <- roc_plot(lda_test_preds, 'LDA')

lda_test_metrics <- test_metrics(lda_test_preds)

lda_test_cm <- conf_mat(lda_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```


```{r figure-X-lda-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Four metrics based on threshold value for the LDA model. Vertical lines show maximum j-index and sensitivity at 0.01.
#| dev: "png"
#| dpi: 100
lda_tmetrics_plot
```
 


```{r lda-cm}
lda_train_cm
```

```{r}
lda_test_cm
```


```{r qda}
# define model spec
qda_spec <- discrim_quad(mode = "classification") %>% 
  set_engine('MASS')

# define workflow
qda_final <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(qda_spec)


# fit resamples
qda_fitcv <- fit_resamples(qda_final, resamples,  control=cv_control)

# collect predictions and roc_auc from cross-validated fit
qda_cv_preds <-  collect_predictions(qda_fitcv)
qda_cv_metrics <- collect_metrics(qda_fitcv)

# get ROC plot for cross-validation
qda_cv_roc <- roc_plot(qda_cv_preds, 'QDA')

# get threshold selection info
qda_thresh_perf <- probably::threshold_perf(qda_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
qda_max_sens <- qda_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
qda_max_j <- qda_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
qda_tmetrics_plot <- threshold_metric_plot(qda_thresh_perf, qda_max_sens, qda_max_j)

# get best threshold
qda_threshold <- as.numeric( (qda_max_sens[1, 1] + qda_max_j[1, 1]) / 2 )

# get predictions and metrics based on chosen threshold
qda_threshold_preds <- qda_cv_preds %>% 
 mutate(.pred_class = factor(ifelse(.pred_Tarp >= qda_threshold, 'Tarp', 'Non-Tarp')))

qda_threshold_metrics <- threshold_metrics(qda_threshold_preds, truth = Class, estimate = .pred_class)


# get confusion matrix for chosen threshold
qda_train_cm <- conf_mat(qda_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
qda_fit <- fit(qda_final, data)

# get predictions and metrics for holdout data
qda_test_preds <- threshold_preds(augment(qda_fit, new_data = holdout),
                                  qda_threshold)
qda_test_roc <- roc_plot(qda_test_preds, 'QDA')

qda_test_metrics <- test_metrics(qda_test_preds)

qda_test_cm <- conf_mat(qda_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```


```{r figure-X-qda-threshold-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: Four metrics based on threshold value for the QDA model. Vertical lines show maximum j-index and sensitivity at 0.02 and 0.01, respectively.
#| dev: "png"
#| dpi: 100
qda_tmetrics_plot
```
 

```{r qda-cm}
qda_train_cm
```

```{r}
qda_test_cm
```

```{r logistic_regression}
log_spec <- logistic_reg(mode = 'classification',
                            engine = 'glm')
                            
# define workflow
log_final <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(log_spec)


# fit resamples
log_fitcv <- fit_resamples(log_final, resamples, control=cv_control)

# collect predictions and roc_auc from cross-validated fit
log_cv_preds <- collect_predictions(log_fitcv)
log_cv_metrics <- collect_metrics(log_fitcv)

# get ROC plot for cross-validation
log_cv_roc <- roc_plot(log_cv_preds, 'Logistic Regression')

# get threshold selection info
log_thresh_perf <- probably::threshold_perf(log_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
log_max_sens <- log_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
log_max_j <- log_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
log_tmetrics_plot <- threshold_metric_plot(log_thresh_perf, log_max_sens, log_max_j)

# get best threshold
log_threshold <- as.numeric(log_max_j[1, 1])

# get predictions and metrics based on chosen threshold
log_threshold_preds <- log_cv_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= log_threshold, 'Tarp', 'Non-Tarp')))

log_threshold_metrics <- threshold_metrics(log_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
log_train_cm <- conf_mat(log_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
log_fit <- fit(log_final, data)

# get predictions and metrics for holdout data
log_test_preds <- threshold_preds(augment(log_fit, new_data = holdout),
                                  log_threshold)
log_test_roc <- roc_plot(log_test_preds, 'Logistic Regression')

log_test_metrics <- test_metrics(log_test_preds)

log_test_cm <- conf_mat(log_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```


```{r penalized_logistic}
# define model spec
pen_spec <- logistic_reg(mode = 'classification',
                            engine = 'glmnet',
                            penalty = tune(),
                            mixture = tune())
                            
# define workflow
pen_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(pen_spec)

# set tuning parameters
pen_params <- extract_parameter_set_dials(pen_wf) %>% 
  update(
    penalty=penalty(c(-20,-1)),
    mixture=mixture(c(0,1))
  )

# tune with grid (or fit resamples)
pen_tune <- tune_grid(pen_wf,
                      resamples=resamples,
                      control=cv_control,
                      grid=grid_latin_hypercube(pen_params, size=50))

# get tuning results visualization
pen_tune_vis <- autoplot(pen_tune, metric = "roc_auc")

# finalize workflow and fit resamples with best parameters
pen_best_params <- select_best(pen_tune, metric="roc_auc")

pen_final <- finalize_workflow(pen_wf, pen_best_params) 

pen_fitcv <- fit_resamples(pen_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
pen_cv_preds <-  collect_predictions(pen_fitcv)
pen_cv_metrics <- collect_metrics(pen_fitcv)

# get ROC plot for cross-validation
pen_cv_roc <- roc_plot(pen_cv_preds, 'Penalized LR')

# get threshold selection info
pen_thresh_perf <- probably::threshold_perf(pen_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
pen_max_sens <- pen_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
pen_max_j <- pen_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
pen_tmetrics_plot <- threshold_metric_plot(pen_thresh_perf, pen_max_sens, pen_max_j)

# get best threshold
pen_threshold <- as.numeric(pen_max_j[1, 1])

# get predictions and metrics based on chosen threshold
pen_threshold_preds <- pen_cv_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= pen_threshold, 'Tarp', 'Non-Tarp')))

pen_threshold_metrics <- threshold_metrics(pen_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
pen_train_cm <- conf_mat(pen_threshold_preds, truth = Class, estimate = .pred_class)

## PART II ##
# fit final workflow to training data
pen_fit <- fit(pen_final, data)

# get predictions and metrics for holdout data
pen_test_preds <- threshold_preds(augment(pen_fit, new_data = holdout),
                                  pen_threshold)
pen_test_roc <- roc_plot(pen_test_preds, 'Penalized Log. Regression')

pen_test_metrics <- test_metrics(pen_test_preds)

pen_test_cm <- conf_mat(pen_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```



```{r figure-X-pen-tune}
#| fig.width: 6
#| fig.height: 3
#| fig.align: center
#| out.width: 100%
#| fig.cap: ROC based on penalty and mixture values. Left hand chart corresponds to penalty value, right hand chart corresponds to mixture.
#| dev: "png"
#| dpi: 100
pen_tune_vis
```




```{r figure-X-log-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Logistic Regression model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.05.
#| dev: "png"
#| dpi: 100
log_tmetrics_plot
```


```{r figure-X-pen-threshold-metrics}
#| fig.width: 5
#| fig.height: 4
#| fig.align: center
#| out.width: 70%
#| fig.cap: Five metrics based on threshold value for the Penalized Logistic Regression model. Vertical lines show maximum j-index and sensitivity between 0.01 and 0.04.
#| dev: "png"
#| dpi: 100
pen_tmetrics_plot
```


The confusion matrices below show the breakdown of each model's predictions at the chosen thresholds.

Standard Logistic Regression:

```{r log-conf-mat}
log_train_cm
```

Penalized Logistic Regression:

```{r pen-conf-mat}
pen_train_cm
```

```{r}
log_test_cm
```

```{r}
pen_test_cm
```

```{r ensemble - boost}
# define model spec

ens_spec <- boost_tree(mode="classification",trees = 10, tree_depth = tune(), learn_rate = tune()) %>%
    set_engine("xgboost")

# define workflow
ens_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(ens_spec)

# set tuning parameters
ens_params <- extract_parameter_set_dials(ens_wf)

# tune with grid (or fit resamples)
ens_tune <- tune_grid(ens_wf,
                      resamples = resamples,
                      grid = grid_regular(ens_params, levels = 10),
                      control = cv_control
                      )

# get tuning results visualization
ens_tune_vis <- autoplot(ens_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
ens_best_params <- select_best(ens_tune, metric = 'roc_auc')

ens_final <- ens_wf %>% 
  finalize_workflow(ens_best_params)

ens_fitcv <- fit_resamples(ens_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
ens_cv_preds <-  collect_predictions(ens_fitcv)
ens_cv_metrics <- collect_metrics(ens_fitcv)

# get ROC plot for cross-validation
ens_cv_roc <- roc_plot(ens_cv_preds, 'Boosting Model')

# get threshold selection info
ens_thresh_perf <- probably::threshold_perf(ens_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
ens_max_sens <- ens_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
ens_max_j <- ens_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
ens_tmetrics_plot <- threshold_metric_plot(ens_thresh_perf, ens_max_sens, ens_max_j)


# get best threshold 
ens_threshold <- as.numeric(ens_max_j[1,1])

# get predictions and metrics based on chosen threshold
ens_threshold_preds <- threshold_preds(ens_cv_preds, ens_threshold)

ens_threshold_metrics <- threshold_metrics(ens_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
ens_train_train_cm <- conf_mat(ens_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##

# fit final workflow to training data
ens_fit <- fit(ens_final,data)

# get predictions and metrics for holdout data
ens_test_preds <- threshold_preds(augment(ens_fit, new_data = holdout),
                                  ens_threshold)
ens_test_roc <- roc_plot(ens_test_preds, "Boost Model")

ens_test_metrics <- test_metrics(ens_test_preds)

ens_test_cm <- conf_mat(ens_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```


```{r}
## PART I ##
# define model spec
svmp_spec <- svm_poly(mode = "classification", engine = "kernlab", cost = tune(),
                        margin = tune(), degree = tune()) 

# define workflow
svmp_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(svmp_spec)

# set tuning parameters
svmp_params <- extract_parameter_set_dials(svmp_wf)

# tune with grid (or fit resamples)
svmp_tune <- tune_bayes(svmp_wf,
                      resamples = resamples,
                      metrics = metric_set(roc_auc),
                      param_info = svmp_params, iter = 20
                      )

# get tuning results visualization
svmp_tune_vis <- autoplot(svmp_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
svmp_best_params <- select_best(svmp_tune, metric = 'roc_auc')

svmp_final <- svmp_wf %>% 
  finalize_workflow(svmp_best_params)

svmp_fitcv <- fit_resamples(svmp_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
svmp_cv_preds <-  collect_predictions(svmp_fitcv)
svmp_cv_metrics <- collect_metrics(svmp_fitcv)

# get ROC plot for cross-validation
svmp_cv_roc <- roc_plot(svmp_cv_preds, 'svmp')

# get threshold selection info
svmp_thresh_perf <- probably::threshold_perf(svmp_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
svmp_max_sens <- svmp_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
svmp_max_j <- svmp_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
svmp_tmetrics_plot <- threshold_metric_plot(svmp_thresh_perf, svmp_max_sens, svmp_max_j)

# get best threshold
svmp_threshold <- as.numeric(svmp_max_sens[1,1])

# get predictions and metrics based on chosen threshold
svmp_threshold_preds <- threshold_preds(svmp_cv_preds, svmp_threshold)

svmp_threshold_metrics <- threshold_metrics(svmp_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
svmp_train_train_cm <- conf_mat(svmp_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##
# fit final workflow to training data
svmp_fit <- fit(svmp_final, data)

# get predictions and metrics for holdout data
svmp_test_preds <- threshold_preds(augment(svmp_fit, new_data = holdout),
                                  svmp_threshold)
svmp_test_roc <- roc_plot(svmp_test_preds, "svmp")

svmp_test_metrics <- test_metrics(svmp_test_preds)

svmp_test_cm <- conf_mat(svmp_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r}
## PART I ##
# define model spec
svmr_spec <- svm_rbf(mode = "classification", engine = "kernlab", cost = tune(),
                        margin = tune(), rbf_sigma = tune()) 

# define workflow
svmr_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(svmr_spec)

# set tuning parameters
svmr_params <- extract_parameter_set_dials(svmr_wf)

# tune with grid (or fit resamples)
svmr_tune <- tune_bayes(svmr_wf,
                      resamples = resamples,
                      metrics = metric_set(roc_auc),
                      param_info = svmr_params, iter = 20
                      )

# get tuning results visualization
svmr_tune_vis <- autoplot(svmr_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
svmr_best_params <- select_best(svmr_tune, metric = 'roc_auc')

svmr_final <- svmr_wf %>% 
  finalize_workflow(svmr_best_params)

svmr_fitcv <- fit_resamples(svmr_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
svmr_cv_preds <-  collect_predictions(svmr_fitcv)
svmr_cv_metrics <- collect_metrics(svmr_fitcv)

# get ROC plot for cross-validation
svmr_cv_roc <- roc_plot(svmr_cv_preds, 'svmr')

# get threshold selection info
svmr_thresh_perf <- probably::threshold_perf(svmr_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
svmr_max_sens <- svmr_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
svmr_max_j <- svmr_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
svmr_tmetrics_plot <- threshold_metric_plot(svmr_thresh_perf, svmr_max_sens, svmr_max_j)

# get best threshold
svmr_threshold <- as.numeric(svmp_max_sens[1,1])

# get predictions and metrics based on chosen threshold
svmr_threshold_preds <- threshold_preds(svmr_cv_preds, svmr_threshold)

svmr_threshold_metrics <- threshold_metrics(svmr_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
svmr_train_train_cm <- conf_mat(svmr_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##
# fit final workflow to training data
svmr_fit <- fit(svmr_final, data)

# get predictions and metrics for holdout data
svmr_test_preds <- threshold_preds(augment(svmr_fit, new_data = holdout),
                                  svmr_threshold)
svmr_test_roc <- roc_plot(svmr_test_preds)

svmr_test_metrics <- test_metrics(svmr_test_preds)

svmr_test_cm <- conf_mat(svmr_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```


```{r compare-models}
# get all cross-validation plots and metrics
cv_rocs <- ( log_cv_roc + pen_cv_roc  + knn_cv_roc) /
  ( lda_cv_roc + qda_cv_roc + ens_cv_roc ) /
  ( svml_cv_roc + svmp_cv_roc + svmr_cv_roc)

compare_cv_metrics <- bind_rows(
  knn_cv_metrics %>% mutate(model = 'KNN'),
  lda_cv_metrics %>% mutate(model = 'LDA'),
  qda_cv_metrics %>% mutate(model = 'QDA'),
  log_cv_metrics %>% mutate(model = 'logreg'),
  pen_cv_metrics %>% mutate(model = 'penalized log'),
  ens_cv_metrics %>% mutate(model = 'ensemble'),
  svml_cv_metrics %>% mutate(model = 'linear SVM'),
  svmp_cv_metrics %>% mutate(model = 'polynomial SVM'),
  svmr_cv_metrics %>% mutate(model = 'radial SVM')
) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = mean) %>% 
  dplyr::select(model, roc_auc)

compare_threshold_metrics <- bind_rows(
  knn_threshold_metrics %>% mutate(model = 'KNN'),
  lda_threshold_metrics %>% mutate(model = 'LDA'),
  qda_threshold_metrics %>% mutate(model = 'QDA'),
  log_threshold_metrics %>% mutate(model = 'logreg'),
  pen_threshold_metrics %>% mutate(model = 'penalized log'),
  ens_threshold_metrics %>% mutate(model = 'ensemble'),
  svml_threshold_metrics %>% mutate(model = 'linear SVM'),
  svmp_threshold_metrics %>% mutate(model = 'polynomial SVM'),
  svmr_threshold_metrics %>% mutate(model = 'radial SVM')
) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = .estimate) %>% 
  mutate(threshold = c(.05, .04, .01, .015, .04)) 

# get all metrics and plots on the holdout dataset
test_rocs <- ( log_test_roc + pen_test_roc  + knn_test_roc) /
  ( lda_test_roc + qda_test_roc + ens_test_roc ) /
  ( svml_test_roc + svmp_test_roc_svmr_test_roc )

compare_test_metrics <- bind_rows(
  knn_test_metrics %>% mutate(model = 'KNN'),
  lda_test_metrics %>% mutate(model = 'LDA'),
  qda_test_metrics %>% mutate(model = 'QDA'),
  log_test_metrics %>% mutate(model = 'logreg'),
  pen_test_metrics %>% mutate(model = 'penalized log'),
  ens_test_metrics %>% mutate(model = 'ensemble'),
  svml_test_metrics %>% mutate(model = 'linear SVM'),
  svmp_test_metrics %>% mutate(model = 'polynomial SVM'),
  svmr_test_metrics %>% mutate(model = 'radial SVM')
) %>% 
  pivot_wider(id_cols = model,
              names_from = .metric,
              values_from = .estimate)

# create point plot to check for overfitting based on ROC AUC and j-index
overfit_plot <- bind_rows(
  compare_cv_metrics %>% 
    dplyr::select(model, roc_auc) %>% 
    mutate(data = 'cv'),
  compare_threshold_metrics %>% 
    dplyr::select(model, j_index) %>% 
    mutate(data = 'train'),
  compare_test_metrics %>% 
    dplyr::select(model, roc_auc, j_index) %>% 
    mutate(data = 'test')
) %>% 
  pivot_longer(cols = c(roc_auc, j_index)) %>% 
  mutate(data = factor(data, levels = c('cv', 'train', 'test'))) %>% 
  ggplot(aes(x = model, y = value, color = data)) +
  geom_point(size = 2) +
  facet_wrap(~name, scales = 'free') +
  labs(x = '', y = 'metric value') +
  theme(axis.text.x = element_text(angle = 30))

```


```{r figure-X-ROC-metrics}
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC plots for all models from top row Logistic, Penalized, KNN  Bottom row LDA, QDA
#| dev: "png"
#| dpi: 100
cv_rocs
```

```{r table-2-metrics}
compare_cv_metrics %>% knitr::kable(caption = "Comparison of cross-validated AUC for all models")
```


```{r table-3-metrics}
compare_threshold_metrics %>% knitr::kable(caption = "Comparison of Threshold Metrics for all models")
```


```{r table-4-metrics}
compare_test_metrics %>% knitr::kable(caption = "Comparison of Metrics on Holdout Dataset for all models")
```



```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
stopCluster(cl)
registerDoSEQ()
```
