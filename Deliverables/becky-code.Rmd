---
title: "Disaster relief project code"
author: "Becky Desrosiers"
date: "2024-02-20"
output: html_document
---

```{r load-packages}
knitr::opts_chunk$set(cahce = TRUE, autodep = TRUE)
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(patchwork)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```

```{r load-data}
# load data from file
# file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
file = '../data.csv'
data <- read_csv(file) %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))

data %>% glimpse()
data %>% summary()
```

```{r split-data}
# split into testing and training set
data_split <- initial_split(data, 0.8, strata = Class)

train <- training(data_split)
test <- testing(data_split)

# define CV resamples
resamples <- vfold_cv(train)
```


```{r prep-wf}
# define formula
formula <- Class ~ Red + Green + Blue

# define recipe
knn_rec <- recipe(formula = formula, data = train) %>% 
  step_pca(all_numeric_predictors(), num_comp = parsnip::tune())

# define model
knn_model <- nearest_neighbor(mode = 'classification',
                            engine = 'kknn',
                            neighbors = parsnip::tune())

# define workflow
knn_wf <- workflow() %>% 
  add_recipe(knn_rec) %>% 
  add_model(knn_model)

# get tuning parameters
parameters <- extract_parameter_set_dials(knn_wf) %>% 
  update(num_comp = num_comp(c(1, 3)),
         neighbors = neighbors(c(2, 50))
  )
```

```{r tune-knn, cache = TRUE}
# tune with grid
knn_tune <- tune_grid(knn_wf,
                      resamples = resamples,
                      grid = grid_latin_hypercube(parameters, size = 30),
                      control = control_resamples(save_pred = TRUE)
                      )
```

## Section 3.5: K-Nearest Neighbors

This section will explore modeling the blue tarp classification with a K-nearest neighbors model. The number of neighbors will be considered for tuning. Dimension reduction will also be considered due to the high correlation between the features. The reason why we choose to tune these parameters is that they are basically the only tunable parameters in a KNN model. Since the available tuning parameters are limited, we can reasonably explore both. We chose principal components instead of partial least squares because of James, et al. assertion that PLS has limited benefit in comparison with PCR or ridge regression.

We use ROC AUC to find the best model because it it is a measure of the model overall, whereas accuracy only measures its performance at a specific chosen threshold. Accuracy will be reported later along with other relevant metrics after a threshold is chosen. We explore from 2 to 50 nearest neighbors and 1 to 3 principal components. Figure 3.X shows the behavior of the ROC AUC based on the number of neighbors and components.


```{r knn-autoplot}
#| fig.width: 9
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC AUC based on number of principal components and nearest neighbors.
#| dev: "png"
#| dpi: 100
# visualize results
autoplot(knn_tune, metric = 'roc_auc')+
  geom_line()
```

```{r knn-show-best}
show_best(knn_tune, metric = 'roc_auc')
```

```{r visualize-bests}
knn_best <- select_by_one_std_err(knn_tune, neighbors, metric = 'roc_auc') %>% 
  dplyr::select(c(neighbors, num_comp))
```

```{r knn-fit-resamples}
knn_fit <- knn_wf %>% 
  finalize_workflow(knn_best) %>% 
  fit_resamples(resamples, control = control_resamples(save_pred = TRUE))
```

The best model based on ROC AUC has 24 neighbors and uses all 3 dimensions. It has a ROC AUC of 0.994. However, there is a simpler model using 8 nearest neighbors that has an ROC AUC of 0.993, which is within one standard error of the best AUC. We will choose this model because it is less complex to check for 8 neighbors than 24 neighbors. Both models use all 3 dimensions.

```{r knn-roc-auc}
#| fig.width: 9
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: ROC AUC for KNN model with 24 neighbors and 3 components based on 10-fold cross validation.
#| dev: "png"
#| dpi: 100
# plot ROC AUC
knn_preds <-  collect_predictions(knn_fit)
knn_preds %>% 
  group_by(id) %>% 
  roc_curve(truth = Class, '.pred_Tarp', event_level = 'second') %>% 
  autoplot() +
  theme(legend.position = 'none')
  
```

Figure 3.X shows the ROC for each of the 10 folds used in cross-validation. The average area under the curve is 0.993. This is an excellent model. If we look at the confusion matrix, however, we see that 76 out of the 1567 total Tarp pixels are missed. In this case, we don't want to miss any tarps, even if it means that we misclassify some Non-Tarp areas as tarps. So we will decrease the threshold to maximize sensitivity and minimize the false negative rate.

```{r knn-cm}
# confusion matrix for threshold = 0.5
conf_mat(knn_preds, truth = Class, estimate = .pred_class)
```

```{r knn-threshold}
knn_thresh_perf <- probably::threshold_perf(knn_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))
```



```{r knn-metric-plot}
max_sens <- knn_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

max_j <- knn_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))
  
```

Figure 3.X shows the accuracy, sensitivity, specificity, and j-index of the model based on different thresholds between 0 and 1. It is vital to the purpose of this investigation to use sensitivity as a metric, because the false negative rate needs to be as minimal as we can make it so that we can find all of the tarps. The vertical lines indicate the maximum values for j-index and sensitivity.

The plot shows that sensitivity is maximized anywhere below threshold = 0.05 and j-index is maximized at threshold = 0.4. Specificity and accuracy are relatively high and constant throughout, and also less important than sensitivity. We will choose threshold = 0.4 to maximize the j-index given sensitivity does not benefit from using a different value.


```{r knn-thresholds}
#| fig.width: 9
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Visualization of ROC AUC based on number of principal components and nearest neighbors.
#| dev: "png"
#| dpi: 100
ggplot(knn_thresh_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
  geom_line() +
  geom_vline(data = max_sens, aes(xintercept = .threshold, color = .metric)) +
  geom_vline(data = max_j, aes(xintercept = .threshold, color = .metric)) +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  labs(x = 'Threshold', y = 'Metric value')

```





```{r knn-conf-mtx}
knn_preds <- knn_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.04, 'Tarp', 'Non-Tarp')))
conf_mat(knn_preds, truth = Class, estimate = .pred_class)
```

After changing the threshold, we only miss 17 Tarp pixels. The model misclassifies more Non-Tarp areas as tarps, as well. In real life, this means that we might send aid to places that don't actually need it. Even so, it is more important for us to make sure we do not overlook actual Blue Tarps where people need help.

```{r knn-final}
# fit the final model with the entire training dataset
knn_final <- knn_wf %>% 
  finalize_workflow(knn_best) %>% 
  fit(train)
```


```{r knn-test-metrics}
get_metrics <- metric_set(j_index, sensitivity, specificity, accuracy)

# get metrics on training data
knn_train_metrics <- knn_preds %>% 
  get_metrics(truth = Class, estimate = .pred_class)

# get metrics on test data
knn_test_metrics <- parsnip::augment(knn_final, new_data = train) %>%
  get_metrics(truth = Class, estimate = .pred_class)

```

The KNN model performs exceedingly well on the cross-validated training data, missing only 1% of blue tarp pixels. Even so, the metrics that matter will be related to the model's performance on the test set. How well can the model classify new pixels? We now apply the model to predict the testing data and the metrics are shown below.


```{r knn-compare-metrics}
bind_rows(knn_train_metrics %>% mutate(data ='Training'),
          knn_test_metrics %>%  mutate(data = 'Testing')) %>% 
  pivot_wider(id_cols =.metric,
              names_from = data,
              values_from = .estimate)
```

The model yields very good metrics on the testing set, showing little evidence of overfitting. Taken on its own, this model would be useful for classifying blue tarps in the pictures.




















