---
title: 'Disaster Relief Project, Part 1'
author: "Becky Desrosiers, Abner Casillas-Colon, Rachel Daniel"
date: "2024-03-16"
output: pdf_document
---

```{r r-setup, include = FALSE, cache = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      autodep = TRUE,
                      message = FALSE,
                      warning = FALSE)
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(discrim)
library(patchwork)
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)
```


# Section 1: Introduction

>What is the key idea for locating people and why do we expect it to work?



# Section 2: Data

```{r load-data, warning = FALSE, message = FALSE}
# load data from file
# file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
file = '../data.csv'
data <- read_csv(file) %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))

data %>% glimpse()
data %>% summary()
```
>You will need to do some EDA
- EDA - focus your discussion of the results on what it means for the model
- Be selective with your EDA. Focus on results that have a relevance to the project. You can always mention that you tried other visualizations, but didn’t get much more out of it. If you add a graph, it should add to your analysis and cover something that cannot already be seen in other graphs.





# Section 3: Description of Methodology

>Overall model building process well defined and explained.
• describe used software
• describe and justify parameter tuning and model selection (if applicable)
• describe and justify model validation
• describe and justify threshold selection
• describe and justify metrics used for model performance evaluation

>Model training consists of these steps:
- Model validation using 10-fold cross validation; this includes model tuning if applicable
- what metric will you use?
- Threshold selection
- what metric will you use?
- Are “penalized logistic regression” and “logistic regression” really two different models or just variations of the same?

>From Becky: be sure to mention we did a 20-80 test-training split with stratified sampling of Blue Tarp pixels.




# Section 4: Results of Model Fitting, Tuning Parameter Selection, and Evaluation


>Model performance summarized in one or more tables or figures. Expected information shown:
• ROC curves and AUC
• Optimal model tuning parameters
• Selected threshold
• Accuracy, TPR, FPR, Precision calculated at selected threshold

>When you present metrics, talk about their relevance for the objective of this project
- When you present your results, split it into results from model validation and results after threshold selection. e.g. ROC curves are only relevant for discussing model validation results and not threshold selection; reporting accuracy or any other threshold dependent metric prior to threshold selection is irrelevant.
- If you visualize the same analysis for each model, combine them either in a single graph or a combination of graphs in one figure (using patchwork)

>Focus your discussion also on differences between models.
- For example LDA and QDA are conceptually similar but differ in the way they can represent the decision boundary. 
- What do the differences in the results for LDA and QDA tell you?
- Another example that you can discuss is the difference between , penalized logistic regression and logistic regression.





```{r convenience-functions}
# define functions to use later for convenience
cv_control <- control_resamples(save_pred = TRUE)

roc_plot <- function(model_preds) {
  model_preds %>% 
    roc_curve(truth = Class,
              .pred_Tarp,
              event_level = 'second') %>% 
    autoplot()
}

threshold_metric_plot <- function(thresh_perf, max_sens, max_j) {
  ggplot(thresh_perf, aes(x = .threshold, y = .estimate, color = .metric)) +
    geom_line() +
    geom_vline(data = max_sens, aes(xintercept = .threshold, color = .metric)) +
    geom_vline(data = max_j, aes(xintercept = .threshold, color = .metric)) +
    scale_x_continuous(breaks = seq(0, 1, 0.1)) +
    labs(x = 'Threshold', y = 'Metric value')
}

threshold_metrics <- metric_set(j_index,
                                specificity,
                                sensitivity,
                                accuracy,
                                precision)


threshold_preds <- function(preds, threshold) {
    preds <- preds %>% 
    mutate(.pred_class = factor(ifelse(.pred_Tarp >= threshold, 'Tarp', 'Non-Tarp')))
}

```


```{r model-setup}
# prepare resamples for 10-fold cross-validation
set.seed(6030)
resamples <- vfold_cv(data, v = 10, strata=Class)

# define formula
formula <- Class ~ Red + Green + Blue

# define basic recipe
rec <- recipe(formula, data = data)
```


## Section 4.1: Logistic Regression Model


```{r knn}
# define model spec
knn_spec <- nearest_neighbor(mode = 'classification',
                            engine = 'kknn',
                            neighbors = parsnip::tune())

# define workflow
knn_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(knn_spec)

# set tuning parameters
knn_params <- extract_parameter_set_dials(knn_wf) %>% 
  update(neighbors = neighbors(c(2, 50)))

# tune with grid (or fit resamples)
knn_fitcv <- tune_grid(knn_wf,
                      resamples = resamples,
                      grid = grid_regular(knn_params, levels = 49),
                      control = cv_control
                      )

# get tuning results visualization
knn_tune_vis <- autoplot(knn_fitcv)$roc_auc

# collect predictions and roc_auc from cross-validated fit
knn_preds <-  collect_predictions(knn_fitcv)
knn_metrics <- collect_metrics(knn_fitcv)

# get ROC plot
knn_roc <- roc_plot(knn_preds)

# get threshold selection info
knn_thresh_perf <- probably::threshold_perf(knn_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = metric_set(j_index,
                                                                 specificity,
                                                                 sensitivity,
                                                                 accuracy))

# get threshold for best sensitivity
knn_max_sens <- knn_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
knn_max_j <- knn_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
knn_tmetrics_plot <- threshold_metric_plot(knn_thresh_perf, knn_max_sens, knn_max_j)

# get predictions and metrics based on chosen threshold
knn_threshold_preds <- knn_preds %>% 
  mutate(.pred_class = factor(ifelse(.pred_Tarp >= 0.04, 'Tarp', 'Non-Tarp')))
knn_threshold_metrics <- threshold_merics(knn_threshold_preds)

# get confusion matrix for chosen threshold
knn_cm <- conf_mat(knn_threshold_preds, truth = Class, estimate = .pred_class)
```


> Write-up


```{r knn-auc-vs-metrics}
#| fig.width: 9
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Plot of ROC curve for the logistic regression model with no penalty. The AUC is 0.9976.
#| dev: "png"
#| dpi: 100
# display logreg ROC curve
roc_plot(logreg_preds)
```

> describe ROC curve, report AUC (without printing it out), and 

Figure 1 shows the corresponding ROC curve to demonstrate the performance of the model. We note a very high AUC of **0.9987**, indicating a very effective model. From the visualization and the ROC AUC metric, it is clear that the model performs well. To optimize its predictive capability, we now investigate further to choose a threshold.












```{r compare-metrics}
compare_metrics <- bind_rows(log_threshold_metrics %>% mutate(model = 'logistic regression'),
                             penlog_threshold_metrics %>% mutate(model = 'penalized logreg'),
                             lda_threshold_metrics %>% mutate(model = 'LDA'),
                             qda_threshold_metrics,
                             knn_threshold_metrics)
```





# Section 5: Conclusion


>Three or more clearly identifiable conclusions. This section is more important than the previous sections (as
reflected in the points). Give sufficient explanation and justification for each conclusion.
One conclusion must be:
• determination and justification of which algorithm works best.
Additional conclusions should be observations you’ve made based on your work on this project, such as:
• What additional recommend actions can be taken to improve results?
• Were there multiple adequately performing methods, or just one clear best method? What is your level
of confidence in the results?
• What is it about this data formulation that allows us to address it with predictive modeling tools?
• How effective do you think your work here could actually be in terms of helping to save human life?
• Do these data seem particularly well-suited to one class of prediction methods, and if so, why?
These are only suggestions, pursue your own interests. Your best two additional conclusions will be graded.
Make sure that the 3 conclusions are clearly separated.










# Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}
stopCluster(cl)
registerDoSEQ()
```
