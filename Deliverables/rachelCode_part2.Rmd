---
title: "Rachel Code_partII"
output:
  pdf_document: default
  html_document: default
date: "2024-04-12"
---

```{r}
library(tidyverse)
library(tidymodels)
library(ggcorrplot)
library(GGally)
library(discrim)
library(patchwork)
library(xgboost) ##need this for boost model
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE))
registerDoParallel(cl)


# load data from file
file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
#file = '../data.csv'
data <- read_csv(file) %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
```


# Data

```{r load-data, include = FALSE}
# load data from file
file = "https://gedeck.github.io/DS-6030/project/HaitiPixels.csv"
#file = '../data.csv'
data <- read_csv(file) #%>% 
  #mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
```

The dataset that will be used consists of class as a response variable and red, blue, green pixels as the predictor variables. There are a total of 63,241 records with no missing values. Pixels can have a range of 0-255 but for this dataset all pixels have a max of 255. Red and green pixels share a minimum value of 48 and blue has a minimum of 44. For Class there are five distinct values: Blue Tarp, Rooftop, Soil, Various Non-Tarp, and Vegetation.

The box plots below showcase the distribution of the range of pixel values grouped by each of the categories of images observed. For our analysis, one thing we would like to note is the potential groups that could overlap with our goal value of blue tarps. The range of pixel values for each color lie within the following approximate ranges for the blue tarp class: 150-200 for the red pixels, 160-250 for the green pixels, and 175-250 for the blue pixels. The blue pixels have the largest average values, which indicates a higher saturation of the color blue (the color of our tarps). 

When viewing the boxplots, there appears to be some overlap of red and green pixel values for the rooftop class and the various non-tarp class with the blue tarp class. The blue pixel values do not overlap as much across classes as the blue tarps have the highest saturation of blue pixels. As we work to identify the best model to identify blue tarps, we will keep in mind that there is some overlap of red and green pixel values of the various non-tarp class and the rooftop class with the blue tarp class. This overlap may potentially make up a portion of the false positives in our final model so we will try to minimize false positives as much as possible while also minimizing false negatives. 

```{r EDA_Boxplots}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each colour and class.
#| dev: "png"
#| dpi: 100
g1_box <- ggplot(data, aes(x=Class, y=Red))+
  geom_boxplot()+
  labs(title= "Boxplots of Red Pixel Value by Class")

g2_box <- ggplot(data, aes(x=Class, y=Green))+
  geom_boxplot()+
  labs(title= "Boxplots of Green Pixel Value by Class")

g3_box <- ggplot(data, aes(x=Class, y=Blue))+
  geom_boxplot()+
  labs(title= "Boxplots of Blue Pixel Value by Class")

g1_box/g2_box/g3_box

```


The count plot below shows the total counts grouped by class. Blue tarp has the smallest amount of values at 2022. While this is important to note for the threshold selection later on in the model, it is promising that the two largest values by far are vegetation and soil as they are the two who are least likely to represent false positive or false negative values when predicting for tarps. Rooftops and Various Non-Tarps however, do account for 14,647 pixels which do have closer overlaps with the green and blue pixel values for blue tarps. Again, we will need to look out for these potential false positives when assessing our models.

```{r EDA_Countplot_Class_ungrouped }
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of total counts broken out by each class
#| dev: "png"
#| dpi: 100
ggplot(data, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Class", x="Count")
```





While it is important to assess red, green, and blue pixel values for each class, our main priority is to identify Blue tarps. Since we are targeting a particular class, we are able to transition into a binary problem where we classify the data as follows: Blue-Tarps for our target class and Non-Tarp for all other classes.  

The bar chart and table 1 below highlight the values of Blue-tarps versus Non-Tarps. In this instance, we note that the proportion of tarp pixels in the total dataset is 3%. When we conduct our threshold selection across all models, this will be important as the models will default to a 50-50 threshold.


```{r}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of Classes: Blue-Tarp and Non-Tarp
#| dev: "png"
#| dpi: 100
data <- data %>% 
  mutate(Class = factor(ifelse(Class == 'Blue Tarp', 'Tarp', 'Non-Tarp')))
ggplot(data, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Classes", x="Count")
```




```{r EDA_Count_Table_1}
table(data$Class) %>% knitr::kable(caption = "Count of Class")
```

In the box plots below, we can again see the overlap of the red and green pixel values of the non-tarp class with the tarp class. Fortunately, there is not as much overlap of the blue pixel values between the tarp class and the non-tarp class. Our best model may potentially focus the blue pixel predictor moreso than the others due to it greater uniqueness.


```{r density}
#| fig.width: 20
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each colour and class.
#| dev: "png"
#| dpi: 200
bluedata <- data[data$Class == 'Tarp', ]
otherdata <- data[data$Class == 'Non-Tarp',]
g1_tarpden <- ggplot(bluedata,aes(x=Red))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Blue-tarps")
g1_nontarpden <- ggplot(otherdata,aes(x=Red))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Non-tarps")
g2_tarpden <- ggplot(bluedata,aes(x=Green))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Blue-tarps")
g2_nontarpden <- ggplot(otherdata,aes(x=Green))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Non-tarps")
g3_tarpden <- ggplot(bluedata,aes(x=Blue))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Blue-tarps")
g3_nontarpden <- ggplot(otherdata,aes(x=Blue))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Non-tarps")

(g1_tarpden+g1_nontarpden)/(g2_tarpden+g2_nontarpden)/(g3_tarpden+g3_nontarpden)
```


```{r}

columns = c('ID', 'X','Y','Map X','Map Y','Lat','Lon','B1','B2','B3')

data_67_BT <- read_table("orthovnir067_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_57_NON <- read_table("orthovnir057_ROI_NON_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_67_NOT <- read_table("orthovnir067_ROI_NOT_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_69_NOT <- read_table("orthovnir069_ROI_NOT_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))

data_69_bt <- read_table("orthovnir069_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_78_bt <- read_table("orthovnir078_ROI_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Tarp"))

data_78_NON <- read_table("orthovnir078_ROI_NON_Blue_Tarps.txt", skip = 8, col_names = columns) %>%
  select(-ID) %>% mutate(Class = as.factor("Non-Tarp"))
  
```
```{r}
data_full <- bind_rows(
  data_67_BT,
  data_57_NON,
  data_67_NOT,
  data_69_NOT,
  data_69_bt,
  data_78_bt,
  data_78_NON)
write.csv(data_full, "Mapping_Data.csv")
```


When we take a look a bar chart of the holdout data below, the difference in the count between the tarps and non-tarps is even more extreme, where the blue-tarps pixels only make up about .7% of the data. 

```{r full binary boxplot}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Count plot of Classes: Blue-Tarp and Non-Tarp
#| dev: "png"
#| dpi: 100
ggplot(data_full, aes(Class))+
  geom_bar()+
  labs(title= "Count Plot of Classes for holdout set", x="Count")
```


```{r}
table(data_full$Class) %>% knitr::kable(caption = "Count of Class for holdout set")
```

In the box plots below of the blue-tarps versus the non-tarps, we can see that the distributions of the pixel values are slightly different between the training data and the hold out set. In assessing our final models, we need to make sure that we try to avoid models that overfit the training data as they may not perform as well on the test data.

```{r density full data}
#| fig.width: 20
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each color and class for the hold-out set.
#| dev: "png"
#| dpi: 200
bluedata_full <- data_full[data_full$Class == 'Tarp', ]
otherdata_full<- data_full[data_full$Class == 'Non-Tarp',]
g1_tarpden_full <- ggplot(bluedata_full,aes(x=B1))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Blue-tarps")
g1_nontarpden_full <- ggplot(otherdata_full,aes(x=B1))+
  geom_boxplot()+
  labs(title= "Box plot of Red Pixel Value of Non-tarps")
g2_tarpden_full <- ggplot(bluedata_full,aes(x=B2))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Blue-tarps")
g2_nontarpden_full <- ggplot(otherdata_full,aes(x=B2))+
  geom_boxplot()+
  labs(title= "Box plot of Green Pixel Value of Non-tarps")
g3_tarpden_full <- ggplot(bluedata_full,aes(x=B3))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Blue-tarps")
g3_nontarpden_full <- ggplot(otherdata_full,aes(x=B3))+
  geom_boxplot()+
  labs(title= "Box plot of Blue Pixel Value of Non-tarps")

(g1_tarpden_full+g1_nontarpden_full)/(g2_tarpden_full+g2_nontarpden_full)/(g3_tarpden_full+g3_nontarpden_full)
```

```{r}
#| fig.width: 20
#| fig.height: 5
#| fig.align: center
#| out.width: 70%
#| fig.cap: Box plots of the pixel values broken out by each color and class for the training data.
#| dev: "png"
#| dpi: 200
(g1_tarpden+g1_nontarpden)/(g2_tarpden+g2_nontarpden)/(g3_tarpden+g3_nontarpden)
```




RANDOM FOREST:

```{r}
## PART I ##
# define model spec
ens_spec <- rand_forest(mode="classification", mtry=tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity")
  
# define workflow
ens_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(ens_spec)

# set tuning parameters

ens_params <- extract_parameter_set_dials(ens_wf) %>% 
  update(mtry = mtry(c(1,3)))

# tune with grid (or fit resamples)
ens_tune <- tune_grid(ens_wf,
                      resamples = resamples,
                      grid = grid_random(ens_params, size = 10),
                      control = cv_control
                      )
# get tuning results visualization
ens_tune_vis <- autoplot(ens_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
ens_best_params <- select_best(ens_tune, metric = 'roc_auc')

ens_final <- ens_wf %>% 
  finalize_workflow(ens_best_params)

ens_fitcv <- fit_resamples(ens_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
ens_cv_preds <-  collect_predictions(ens_fitcv)
ens_cv_metrics <- collect_metrics(ens_fitcv)

# get ROC plot for cross-validation
ens_cv_roc <- roc_plot(ens_cv_preds, 'Random Forest')

# get threshold selection info
ens_thresh_perf <- probably::threshold_perf(ens_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
ens_max_sens <- ens_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
ens_max_j <- ens_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
ens_tmetrics_plot <- threshold_metric_plot(ens_thresh_perf, ens_max_sens, ens_max_j)



# get best threshold. 

ens_threshold <- as.numeric(ens_max_j[1])

# get predictions and metrics based on chosen threshold
ens_threshold_preds <- threshold_preds(ens_cv_preds, ens_threshold)

ens_threshold_metrics <- threshold_metrics(ens_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
ens_train_cm <- conf_mat(ens_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##

# fit final workflow to training data - will we change this variable name so its more clear?
ens_fit <- fit(ens_final, data)


# get predictions and metrics for holdout data
ens_test_preds <- threshold_preds(augment(ens_fit, new_data = holdout),
                                  ens_threshold)
ens_test_roc <- roc_plot(ens_test_preds, "Random Forest")

ens_test_metrics <- test_metrics(ens_test_preds)

ens_test_cm <- conf_mat(ens_test_preds,
                        truth = Class,
                        estimate = .pred_class)

```


BOOSTING:
```{r boosting}
## PART I ##

# define model spec

boost_spec <- boost_tree(mode="classification",trees = 10, tree_depth = tune(), learn_rate = tune()) %>%
    set_engine("xgboost")

# define workflow
boost_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(boost_spec)

# set tuning parameters
boost_params <- extract_parameter_set_dials(boost_wf)

# tune with grid (or fit resamples)
boost_tune <- tune_grid(boost_wf,
                      resamples = resamples,
                      grid = grid_regular(boost_params, levels = 10),
                      control = cv_control
                      )

# get tuning results visualization
boost_tune_vis <- autoplot(boost_tune, metric = 'roc_auc')

# finalize workflow and fit resamples with best parameters
boost_best_params <- select_best(boost_tune, metric = 'roc_auc')

boost_final <- boost_wf %>% 
  finalize_workflow(boost_best_params)

boost_fitcv <- fit_resamples(boost_final,
                           resamples,
                           control = cv_control
                           )

# collect predictions and roc_auc from cross-validated fit
boost_cv_preds <-  collect_predictions(boost_fitcv)
boost_cv_metrics <- collect_metrics(boost_fitcv)

# get ROC plot for cross-validation
boost_cv_roc <- roc_plot(boost_cv_preds, 'Boosting Model')

# get threshold selection info
boost_thresh_perf <- probably::threshold_perf(boost_cv_preds,
                                            Class,
                                            .pred_Tarp,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = 'second',
                                            metrics = threshold_metrics)

# get threshold for best sensitivity
boost_max_sens <- boost_thresh_perf %>% 
  filter(.metric == 'sensitivity') %>% 
  filter(.estimate == max(.estimate))

# get threshold for best j-index
boost_max_j <- boost_thresh_perf %>% 
  filter(.metric == 'j_index') %>% 
  filter(.estimate == max(.estimate))

# get threshold metrics plot
boost_tmetrics_plot <- threshold_metric_plot(boost_thresh_perf, boost_max_sens, boost_max_j)


# get best threshold 
boost_threshold <- as.numeric(boost_max_j[1])

# get predictions and metrics based on chosen threshold
boost_threshold_preds <- threshold_preds(boost_cv_preds, boost_threshold)

boost_threshold_metrics <- threshold_metrics(boost_threshold_preds,
                                           truth = Class,
                                           estimate = .pred_class)

# get confusion matrix for chosen threshold
boost_train_train_cm <- conf_mat(boost_threshold_preds,
                               truth = Class,
                               estimate = .pred_class)

## PART II ##

# fit final workflow to training data
boost_fit <- fit(boost_final,holdout)

# get predictions and metrics for holdout data
boost_test_preds <- threshold_preds(augment(boost_fit, new_data = holdout),
                                  boost_threshold)
boost_test_roc <- roc_plot(boost_test_preds, "Boost Model")

boost_test_metrics <- test_metrics(boost_test_preds)

boost_test_cm <- conf_mat(boost_test_preds,
                        truth = Class,
                        estimate = .pred_class)
```

```{r}
stopCluster(cl)
registerDoSEQ()
```